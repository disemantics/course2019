{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "# NSU Distributional Semantics 2019 Course. Seminar 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4f108fcd7e78fa0c90ec7bfa1e3b9e7e13532a4a"
   },
   "source": [
    "Our course repo: https://github.com/disemantics/course2019\n",
    "\n",
    "![](http://imgur.com/S8WgwBp.png)\n",
    "\n",
    "In this seminal, we will learn how to implement and use topic modeling algorithms. We will consider **LSA** (latent semantic analysis), **PLSA** (probabilistic latent semantic analysis) and **LDA** (latent Dirichlet allocation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "875299ffc8fbe124c16bd9b74971bfce365c0923"
   },
   "source": [
    "## Reading data\n",
    "At first, we need to open load a dataset and select text colomns from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3e91cad5987fa8148772df8eabf9c57f7ff1a9a2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d346265bbe8bd17d57c8fd1d8967fba3c862ce83"
   },
   "outputs": [],
   "source": [
    "# If you are working in Colab, run:\n",
    "#!wget https://github.com/disemantics/course2019/raw/master/week03/data/hillary-clinton-emails.zip\n",
    "#!unzip ./hillary-clinton-emails.zip -d ./\n",
    "#data_path = \"./Emails.csv\"\n",
    "# Otherwise, run:\n",
    "!unzip ./data/hillary-clinton-emails.zip -d ./data/\n",
    "data_path = \"./data/Emails.csv\"\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e41599b26459545cfa2c264745af77d83d555500"
   },
   "outputs": [],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "53acb27459d906b6614a477f99e60ff99b96804d"
   },
   "outputs": [],
   "source": [
    "print(f\"Number of Emails: {data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "95ff21bee80d430776f4667ec7980f1892773190"
   },
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7b76d221133af70a4bc0a027d2b818f2df3b6cca"
   },
   "source": [
    "We select only the main text column (ExtractedBodyText) witiout NaNs (empty emails)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "29080f709db703bec01b9b799064f467f65ee9cb"
   },
   "outputs": [],
   "source": [
    "data = data[pd.notnull(data['ExtractedBodyText'])]\n",
    "print(data.sample(5)['ExtractedBodyText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "99181eeaa87c1710a42448a63fdd013210115879"
   },
   "outputs": [],
   "source": [
    "print(f\"Number of Emails: {data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "07e92c0c020fbc4f3a1df01bef3ddbe19f377889"
   },
   "source": [
    "On the next step, we need to clear our data from punctuation and stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "78e7696cc3e07cc2814d14dea6f8d404bab80b7b"
   },
   "outputs": [],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c68ccfa8362d84b6d25540037489828486d6cf38"
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "texts = [tokenizer.tokenize(email.lower()) for email in data['ExtractedBodyText']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "60d7726278aa3b340a02a99ab88a41b35ccffb55"
   },
   "outputs": [],
   "source": [
    "print(texts[5070])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ddb0f829f032d0eb3605f66667927853eddb05ca"
   },
   "outputs": [],
   "source": [
    "def delete_stopwords(tokenized_sentence: list):\n",
    "    return list(filter(lambda x: x not in stop_words, tokenized_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0126e409a17dc8c71a8187018560be475c345b3a"
   },
   "outputs": [],
   "source": [
    "texts = list(filter(lambda x: len(x) > 5, [delete_stopwords(text) for text in texts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "04450ead407217ee88f80a11f95fea1ea5861de5"
   },
   "outputs": [],
   "source": [
    "print(f\"Number of Emails: {len(texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5d741ee40237b60e2bbb42fd13f1179b5307eaef"
   },
   "source": [
    "Now it's time to convert texts to bag-of-words format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d601c98493043873325aa42ffbb94f8137cb14fe"
   },
   "outputs": [],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8237b4cc2a32876ff9a37f3750cba219d95a6efe"
   },
   "outputs": [],
   "source": [
    "corpora_dict = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "29b0ab020048d8cf1ae425809ec8c34ecce703ef"
   },
   "outputs": [],
   "source": [
    "print(list(corpora_dict.token2id.items())[::500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "0467276cd94e04b44c8ffca7a43061f8c0c19a1c"
   },
   "outputs": [],
   "source": [
    "corpora_dict[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "14be222a1976cff15c5bfdbfaa41ba2a034d38d0"
   },
   "outputs": [],
   "source": [
    "corpora_dict.id2token[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f9a5ec4331288accb9b6f33fce35442a1c15edba"
   },
   "outputs": [],
   "source": [
    "corpus = [corpora_dict.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "dde312d3412334fbaff71777d951d5c28b7b0c14"
   },
   "outputs": [],
   "source": [
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dfbe1e8e8a46eb1f15c88ed57ac7c02fc60b43b8"
   },
   "source": [
    "## LSI (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5b55131feab129ca452be8e43c1577495f9ba0da"
   },
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cce5e8698535e85a7e3a77841b614e39e833142b"
   },
   "outputs": [],
   "source": [
    "model_lsi = LsiModel(corpus, id2word=corpora_dict.id2token, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b9e51f95b5fd962ac3011ccf874827981413d4bc"
   },
   "outputs": [],
   "source": [
    "str_topics = [topic_w for topic_number, topic_w in model_lsi.print_topics()]\n",
    "str_topics_split = list(map(lambda x: x.split(\"+\"), str_topics))\n",
    "str_topics_split = [list(map(lambda x: x.split(\"*\")[1].strip()[1:-1], elem)) for elem in str_topics_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b42f9b51853efcdfe153d1dff089f602fd0998cf"
   },
   "outputs": [],
   "source": [
    "for topic in str_topics_split:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e3e3fa8c76f584360562dd018d7e8b11af43a024"
   },
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5a418974a9dbe3dd525cefaa20a0a5b57bce3d47"
   },
   "outputs": [],
   "source": [
    "from gensim import matutils\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6f2baa247a875787ea2ae04ec15b0eedbb4edf77"
   },
   "outputs": [],
   "source": [
    "model_lda = LdaModel(corpus, passes=20, num_topics=10, id2word=corpora_dict.id2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7b97b8291440c64711fe1bf223d51b6861bcf736"
   },
   "outputs": [],
   "source": [
    "str_topics = [topic_w for topic_number, topic_w in model_lda.print_topics()]\n",
    "str_topics_split = list(map(lambda x: x.split(\"+\"), str_topics))\n",
    "str_topics_split = [list(map(lambda x: x.split(\"*\")[1].strip()[1:-1], elem)) for elem in str_topics_split]\n",
    "\n",
    "for topic in str_topics_split:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5d20f76d262a630de2a0f093c64f74828372a25d"
   },
   "outputs": [],
   "source": [
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2e137e6ad8462c8fdd8a166f6bf859404057d66b"
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "data_lda = pyLDAvis.gensim.prepare(model_lda, corpus, corpora_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "121b1678ee832ce81b4e2266ec1cc9d5470934df"
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(data_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ddedbec170feaabf7bc4accae61eaa9e0f637f92"
   },
   "source": [
    "# Homework (10 points)\n",
    "## PLSA\n",
    "Implement a PLSA model with gensim-like interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "73a8ba1ec92ba058c262ff1f6849794b5346bae1"
   },
   "outputs": [],
   "source": [
    "from gensim.matutils import corpus2dense\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c7df8b77fee1cfa0236ad7cedd1af2f973a6e233"
   },
   "source": [
    "This pseudocode of EM-algorithm is taken from here (http://www.machinelearning.ru/wiki/images/8/88/Voron-iip9-talk.pdf) (p. 10):\n",
    "\n",
    "1. initialize $\\mathbf{\\Phi}$ and $\\mathbf{\\Theta}$ so that $\\forall z \\in \\mathcal{Z}\\,\\,\\, \\sum \\limits_{w \\in \\mathcal{W}} \\Phi_{w, z} = 1,\\,\\Phi_{w, z} \\geq 0,\\,\\, \\forall d \\in \\mathcal{D} \n",
    "    \\sum \\limits_{z \\in \\mathcal{Z}} \\Theta_{z, d} = 1,\\, \\Theta_{z, d} \\geq 0.$ \n",
    "2. $passes$ times repeat\n",
    "3. $\\quad \\forall w \\in \\mathcal{W}, z \\in \\mathcal{Z}\\,n_{wz} := 0,\\,n_{zd} := 0,\\,n_z := 0$.\n",
    "4. $\\quad \\forall d \\in \\mathcal{D},\\,w \\in d$:\n",
    "5. $\\quad \\quad Z_w = \\sum\\limits_{z \\in \\mathcal{Z}} \\Phi_{w,z} \\Theta_{z,d}$\n",
    "6. $\\quad \\quad \\forall z \\in \\mathcal{Z}$ such that  $\\Phi_{w,z} \\Theta_{z,d} > 0$\n",
    "7. $\\quad \\quad \\quad$ add $\\frac{n_{wd}}{Z_w} \\Phi_{w,z} \\Theta_{z,d}$ to $\\,n_{wz},\\,n_{zd} ,\\,n_z$\n",
    "8. $\\quad \\forall w \\in \\mathcal{W}, z \\in \\mathcal{Z}\\,\\, \\Phi_{w,z} := n_{wz} / n_{z} $\n",
    "9. $\\quad \\forall d \\in \\mathcal{D}, z \\in \\mathcal{Z}\\,\\, \\Theta_{z,d} := n_{zd} / n_{d} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "af87733cdb62fa863f68db2dd9048d2a14415d24"
   },
   "outputs": [],
   "source": [
    "class PlsaModel:\n",
    "    def __init__(self, corpus=None, id2word=None, num_topics=10, passes=30):\n",
    "        self.passes = passes\n",
    "        \n",
    "        self.num_topics = num_topics\n",
    "        self.num_documents = len(corpus)\n",
    "        self.num_words = len(id2word)\n",
    "\n",
    "        self.id2word = id2word\n",
    "        \n",
    "        self.n_wd = corpus2dense(corpus, num_terms=self.num_words)  # [word][document]\n",
    "        self.n_d = np.sum(self.n_wd, axis=0)\n",
    "        self.n = np.sum(self.n_d)\n",
    "\n",
    "        self.phi = np.random.random_sample(size=(self.num_words, self.num_topics))\n",
    "        self.phi /= np.sum(self.phi, axis=0)\n",
    "        self.theta_t = np.random.random_sample(size=(self.num_documents, self.num_topics))\n",
    "        self.theta_t /= np.sum(self.theta_t, axis=1)[:, None]\n",
    "        \n",
    "        for i in range(self.passes):\n",
    "            self._fit()\n",
    "\n",
    "    def _fit(self):\n",
    "        # n_zd = # YOUR CODE HERE\n",
    "        # n_wz = # YOUR CODE HERE\n",
    "        # n_z = # YOUR CODE HERE\n",
    "        for d in range(self.num_documents):\n",
    "            # YOUR CODE HERE\n",
    "            pass\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "    def print_topics(self, top_n=10):\n",
    "        res = []\n",
    "        for t in range(self.num_topics):\n",
    "            top_inds = self.phi[:, t].argsort()[-top_n:][::-1]\n",
    "            top_words = [self.id2word[x] for x in top_inds]\n",
    "            res.append(top_words)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c5caf7f6d7700f4215ce355f57760711dd865d41"
   },
   "outputs": [],
   "source": [
    "model_plsa = PlsaModel(corpus, passes=10, num_topics=10, id2word=corpora_dict.id2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d702d85c861448a6e97068125de2cc62d0efc495"
   },
   "outputs": [],
   "source": [
    "for topic in model_plsa.print_topics():\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "598ae950aded743573aa56b58788a4969286a8c1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
