{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7911626d589355c4cf273c487f1c995a456cf609"
   },
   "source": [
    "# NSU Distributional Semantics 2019 Course. Seminar 4¶\n",
    "\n",
    "In this seminar, we will see how to train a Word2Vec model and then how to build a sentiment analyser for tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "1689dc37cf4c2038bcda55e2d9b26c0686fb6824"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "01bc4492a6e235967337bcb650a93b7c67135cc9"
   },
   "outputs": [],
   "source": [
    "#data_url = 'https://www.gutenberg.org/files/1703/1703-0.txt'\n",
    "#text = requests.get(data_url).text.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "0e3b748f5a593eda7bc709800651ec507178df65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nips-2015-papers  twitter-emotion\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../input/\n",
    "data = pd.read_csv('../input/nips-2015-papers/Papers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "e4ea6ec4d483c69f2119903abbe93eaa31a10163"
   },
   "outputs": [],
   "source": [
    "#Create a function which return lines preprocessed thanks to gensim.utils.simple_preprocess\n",
    "\n",
    "def read_line(file):\n",
    "  ratio = len(file)/100\n",
    "  for i,line in enumerate(file):\n",
    "    if (i % ratio == 0): logging.info(\"read {0} lines\".format(i))\n",
    "    yield gensim.utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "0dcfd2c558e42c729b8dbe98b526fd24e9265cf9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-17 16:48:26,870 : INFO : read 0 lines\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['learning', 'with', 'symmetric', 'label', 'noise', 'the', 'importance', 'of', 'being', 'unhinged', 'brendan', 'van', 'rooyen', 'aditya', 'krishna', 'menon', 'the', 'australian', 'national', 'university', 'robert', 'williamson', 'national', 'ict', 'australia', 'brendan', 'vanrooyen', 'aditya', 'menon', 'bob', 'williamson', 'nicta', 'com', 'au', 'abstract', 'convex', 'potential', 'minimisation', 'is', 'the', 'de', 'facto', 'approach', 'to', 'binary', 'classification', 'however', 'long', 'and', 'servedio', 'proved', 'that', 'under', 'symmetric', 'label', 'noise', 'sln', 'minimisation', 'of', 'any', 'convex', 'potential', 'over', 'linear', 'function', 'class', 'can', 'result', 'in', 'classification', 'performance', 'equivalent', 'to', 'random', 'guessing', 'this', 'ostensibly', 'shows', 'that', 'convex', 'losses', 'are', 'not', 'sln', 'robust', 'in', 'this', 'paper', 'we', 'propose', 'convex', 'classification', 'calibrated', 'loss', 'and', 'prove', 'that', 'it', 'is', 'sln', 'robust', 'the', 'loss', 'avoids', 'the', 'long', 'and', 'servedio', 'result', 'by', 'virtue', 'of', 'being', 'negatively', 'unbounded', 'the', 'loss', 'is', 'modification', 'of', 'the', 'hinge', 'loss', 'where', 'one', 'does', 'not', 'clamp', 'at', 'zero', 'hence', 'we', 'call', 'it', 'the', 'unhinged', 'loss', 'we', 'show', 'that', 'the', 'optimal', 'unhinged', 'solution', 'is', 'equivalent', 'to', 'that', 'of', 'strongly', 'regularised', 'svm', 'and', 'is', 'the', 'limiting', 'solution', 'for', 'any', 'convex', 'potential', 'this', 'implies', 'that', 'strong', 'regularisation', 'makes', 'most', 'standard', 'learners', 'sln', 'robust', 'experiments', 'confirm', 'the', 'unhinged', 'loss', 'sln', 'robustness', 'is', 'borne', 'out', 'in', 'practice', 'so', 'with', 'apologies', 'to', 'wilde', 'while', 'the', 'truth', 'is', 'rarely', 'pure', 'it', 'can', 'be', 'simple', 'learning', 'with', 'symmetric', 'label', 'noise', 'binary', 'classification', 'is', 'the', 'canonical', 'supervised', 'learning', 'problem', 'given', 'an', 'instance', 'space', 'and', 'samples', 'from', 'some', 'distribution', 'over', 'the', 'goal', 'is', 'to', 'learn', 'scorer', 'with', 'low', 'error', 'on', 'future', 'samples', 'drawn', 'from', 'our', 'interest', 'is', 'in', 'the', 'more', 'realistic', 'scenario', 'where', 'the', 'learner', 'observes', 'samples', 'from', 'some', 'corruption', 'of', 'where', 'labels', 'have', 'some', 'constant', 'probability', 'of', 'being', 'flipped', 'and', 'the', 'goal', 'is', 'still', 'to', 'perform', 'well', 'with', 'respect', 'to', 'this', 'problem', 'is', 'known', 'as', 'learning', 'from', 'symmetric', 'label', 'noise', 'sln', 'learning', 'angluin', 'and', 'laird', 'long', 'and', 'servedio', 'showed', 'that', 'there', 'exist', 'linearly', 'separable', 'where', 'when', 'the', 'learner', 'observes', 'some', 'corruption', 'with', 'symmetric', 'label', 'noise', 'of', 'any', 'nonzero', 'rate', 'minimisation', 'of', 'any', 'convex', 'potential', 'over', 'linear', 'function', 'class', 'results', 'in', 'classification', 'performance', 'on', 'that', 'is', 'equivalent', 'to', 'random', 'guessing', 'ostensibly', 'this', 'establishes', 'that', 'convex', 'losses', 'are', 'not', 'sln', 'robust', 'and', 'motivates', 'the', 'use', 'of', 'non', 'convex', 'losses', 'stempfel', 'and', 'ralaivola', 'masnadi', 'shirazi', 'et', 'al', 'ding', 'and', 'vishwanathan', 'denchev', 'et', 'al', 'manwani', 'and', 'sastry', 'in', 'this', 'paper', 'we', 'propose', 'convex', 'loss', 'and', 'prove', 'that', 'it', 'is', 'sln', 'robust', 'the', 'loss', 'avoids', 'the', 'result', 'of', 'long', 'and', 'servedio', 'by', 'virtue', 'of', 'being', 'negatively', 'unbounded', 'the', 'loss', 'is', 'modification', 'of', 'the', 'hinge', 'loss', 'where', 'one', 'does', 'not', 'clamp', 'at', 'zero', 'thus', 'we', 'call', 'it', 'the', 'unhinged', 'loss', 'this', 'loss', 'has', 'several', 'appealing', 'properties', 'such', 'as', 'being', 'the', 'unique', 'convex', 'loss', 'satisfying', 'notion', 'of', 'strong', 'sln', 'robustness', 'proposition', 'being', 'classification', 'calibrated', 'proposition', 'consistent', 'when', 'minimised', 'on', 'proposition', 'and', 'having', 'an', 'simple', 'optimal', 'solution', 'that', 'is', 'the', 'difference', 'of', 'two', 'kernel', 'means', 'equation', 'finally', 'we', 'show', 'that', 'this', 'optimal', 'solution', 'is', 'equivalent', 'to', 'that', 'of', 'strongly', 'regularised', 'svm', 'proposition', 'and', 'any', 'twice', 'differentiable', 'convex', 'potential', 'proposition', 'implying', 'that', 'strong', 'regularisation', 'endows', 'most', 'standard', 'learners', 'with', 'sln', 'robustness', 'the', 'classifier', 'resulting', 'from', 'minimising', 'the', 'unhinged', 'loss', 'is', 'not', 'new', 'devroye', 'et', 'al', 'chapter', 'scho', 'lkopf', 'and', 'smola', 'section', 'shawe', 'taylor', 'and', 'cristianini', 'section', 'however', 'establishing', 'this', 'classifier', 'strong', 'sln', 'robustness', 'uniqueness', 'thereof', 'and', 'its', 'equivalence', 'to', 'highly', 'regularised', 'svm', 'solution', 'to', 'our', 'knowledge', 'is', 'novel', 'background', 'and', 'problem', 'setup', 'fix', 'an', 'instance', 'space', 'we', 'denote', 'by', 'distribution', 'over', 'with', 'random', 'variables', 'any', 'may', 'be', 'expressed', 'via', 'the', 'class', 'conditionals', 'and', 'base', 'rate', 'or', 'via', 'the', 'marginal', 'and', 'class', 'probability', 'function', 'we', 'interchangeably', 'write', 'as', 'dp', 'or', 'dm', 'classifiers', 'scorers', 'and', 'risks', 'scorer', 'is', 'any', 'function', 'loss', 'is', 'any', 'function', 'we', 'use', 'to', 'refer', 'to', 'and', 'the', 'conditional', 'risk', 'is', 'defined', 'as', 'given', 'distribution', 'the', 'risk', 'of', 'scorer', 'is', 'defined', 'as', 'ld', 'so', 'that', 'ld', 'for', 'set', 'is', 'the', 'set', 'of', 'risks', 'for', 'all', 'scorers', 'in', 'function', 'class', 'is', 'any', 'rx', 'given', 'some', 'the', 'set', 'of', 'restricted', 'bayes', 'optimal', 'scorers', 'for', 'loss', 'are', 'those', 'scorers', 'in', 'that', 'minimise', 'the', 'risk', 'sd', 'argmin', 'ld', 'the', 'set', 'of', 'unrestricted', 'bayes', 'optimal', 'scorers', 'is', 'sd', 'sd', 'for', 'rx', 'the', 'restricted', 'regret', 'of', 'scorer', 'is', 'its', 'excess', 'risk', 'over', 'that', 'of', 'any', 'restricted', 'bayes', 'optimal', 'scorer', 'regretd', 'ld', 'inf', 'binary', 'classification', 'is', 'concerned', 'with', 'the', 'zero', 'one', 'loss', 'jyv', 'jv', 'loss', 'is', 'classification', 'calibrated', 'if', 'all', 'its', 'bayes', 'optimal', 'scorers', 'are', 'also', 'optimal', 'for', 'zero', 'one', 'loss', 'sd', 'sd', 'convex', 'potential', 'is', 'any', 'loss', 'yv', 'where', 'is', 'convex', 'non', 'increasing', 'differentiable', 'with', 'and', 'long', 'and', 'servedio', 'definition', 'all', 'convex', 'potentials', 'are', 'classification', 'calibrated', 'bartlett', 'et', 'al', 'theorem', 'learning', 'with', 'symmetric', 'label', 'noise', 'sln', 'learning', 'the', 'problem', 'of', 'learning', 'with', 'symmetric', 'label', 'noise', 'sln', 'learning', 'is', 'the', 'following', 'angluin', 'and', 'laird', 'kearns', 'blum', 'and', 'mitchell', 'natarajan', 'et', 'al', 'for', 'some', 'notional', 'clean', 'distribution', 'which', 'we', 'would', 'like', 'to', 'observe', 'we', 'instead', 'observe', 'samples', 'from', 'some', 'corrupted', 'distribution', 'sln', 'for', 'some', 'the', 'distribution', 'sln', 'is', 'such', 'that', 'the', 'marginal', 'distribution', 'of', 'instances', 'is', 'unchanged', 'but', 'each', 'label', 'is', 'independently', 'flipped', 'with', 'probability', 'the', 'goal', 'is', 'to', 'learn', 'scorer', 'from', 'these', 'corrupted', 'samples', 'such', 'that', 'ld', 'is', 'small', 'for', 'any', 'quantity', 'in', 'we', 'denote', 'its', 'corrupted', 'counterparts', 'in', 'sln', 'with', 'bar', 'for', 'the', 'corrupted', 'marginal', 'distribution', 'and', 'for', 'the', 'corrupted', 'class', 'probability', 'function', 'additionally', 'when', 'is', 'clear', 'from', 'context', 'we', 'will', 'occasionally', 'refer', 'to', 'sln', 'by', 'it', 'is', 'easy', 'to', 'check', 'that', 'the', 'corrupted', 'marginal', 'distribution', 'and', 'natarajan', 'et', 'al', 'lemma', 'sln', 'robustness', 'formalisation', 'we', 'consider', 'learners', 'for', 'loss', 'and', 'function', 'class', 'with', 'learning', 'being', 'the', 'search', 'for', 'some', 'that', 'minimises', 'the', 'risk', 'informally', 'is', 'robust', 'to', 'symmetric', 'label', 'noise', 'slnrobust', 'if', 'minimising', 'over', 'gives', 'the', 'same', 'classifier', 'on', 'both', 'the', 'clean', 'distribution', 'which', 'the', 'learner', 'would', 'like', 'to', 'observe', 'and', 'sln', 'for', 'any', 'which', 'the', 'learner', 'actually', 'observes', 'we', 'now', 'formalise', 'this', 'notion', 'and', 'review', 'what', 'is', 'known', 'about', 'sln', 'robust', 'learners', 'sln', 'robust', 'learners', 'formal', 'definition', 'for', 'some', 'fixed', 'instance', 'space', 'let', 'denote', 'the', 'set', 'of', 'distributions', 'on', 'given', 'notional', 'clean', 'distribution', 'nsln', 'returns', 'the', 'set', 'of', 'possible', 'corrupted', 'versions', 'of', 'the', 'learner', 'may', 'observe', 'where', 'labels', 'are', 'flipped', 'with', 'unknown', 'probability', 'nsln', 'sln', 'equipped', 'with', 'this', 'we', 'define', 'our', 'notion', 'of', 'sln', 'robustness', 'definition', 'sln', 'robustness', 'we', 'say', 'that', 'learner', 'is', 'sln', 'robust', 'if', 'nsln', 'ld', 'ld', 'that', 'is', 'sln', 'robustness', 'requires', 'that', 'for', 'any', 'level', 'of', 'label', 'noise', 'in', 'the', 'observed', 'distribution', 'the', 'classification', 'performance', 'wrt', 'of', 'the', 'learner', 'is', 'the', 'same', 'as', 'if', 'the', 'learner', 'directly', 'observes', 'unfortunately', 'widely', 'adopted', 'class', 'of', 'learners', 'is', 'not', 'sln', 'robust', 'as', 'we', 'will', 'now', 'see', 'convex', 'potentials', 'with', 'linear', 'function', 'classes', 'are', 'not', 'sln', 'robust', 'fix', 'rd', 'and', 'consider', 'learners', 'with', 'convex', 'potential', 'and', 'function', 'class', 'of', 'linear', 'scorers', 'flin', 'hw', 'xi', 'rd', 'this', 'captures', 'the', 'linear', 'svm', 'and', 'logistic', 'regression', 'which', 'are', 'widely', 'studied', 'in', 'theory', 'and', 'applied', 'in', 'practice', 'disappointingly', 'these', 'learners', 'are', 'not', 'sln', 'robust', 'long', 'and', 'servedio', 'theorem', 'give', 'an', 'example', 'where', 'when', 'learning', 'under', 'symmetric', 'label', 'noise', 'for', 'any', 'convex', 'potential', 'the', 'corrupted', 'risk', 'minimiser', 'over', 'flin', 'has', 'classification', 'performance', 'equivalent', 'to', 'random', 'guessing', 'on', 'this', 'implies', 'that', 'flin', 'is', 'not', 'sln', 'robust', 'as', 'per', 'definition', 'proposition', 'long', 'and', 'servedio', 'theorem', 'let', 'rd', 'for', 'any', 'pick', 'any', 'convex', 'potential', 'then', 'flin', 'is', 'not', 'sln', 'robust', 'the', 'fallout', 'what', 'learners', 'are', 'sln', 'robust', 'in', 'light', 'of', 'proposition', 'there', 'are', 'two', 'ways', 'to', 'proceed', 'in', 'order', 'to', 'obtain', 'sln', 'robust', 'learners', 'either', 'we', 'change', 'the', 'class', 'of', 'losses', 'or', 'we', 'change', 'the', 'function', 'class', 'the', 'first', 'approach', 'has', 'been', 'pursued', 'in', 'large', 'body', 'of', 'work', 'that', 'embraces', 'non', 'convex', 'losses', 'stempfel', 'and', 'ralaivola', 'masnadi', 'shirazi', 'et', 'al', 'ding', 'and', 'vishwanathan', 'denchev', 'et', 'al', 'manwani', 'and', 'sastry', 'while', 'such', 'losses', 'avoid', 'the', 'conditions', 'of', 'proposition', 'this', 'does', 'not', 'automatically', 'imply', 'that', 'they', 'are', 'sln', 'robust', 'when', 'used', 'with', 'flin', 'in', 'appendix', 'we', 'present', 'evidence', 'that', 'some', 'of', 'these', 'losses', 'are', 'in', 'fact', 'not', 'sln', 'robust', 'when', 'used', 'with', 'flin', 'the', 'second', 'approach', 'is', 'to', 'consider', 'suitably', 'rich', 'that', 'contains', 'the', 'bayes', 'optimal', 'scorer', 'for', 'by', 'employing', 'universal', 'kernel', 'with', 'this', 'choice', 'one', 'can', 'still', 'use', 'convex', 'potential', 'loss', 'and', 'in', 'fact', 'owing', 'to', 'equation', 'any', 'classification', 'calibrated', 'loss', 'proposition', 'pick', 'any', 'classification', 'calibrated', 'then', 'rx', 'is', 'sln', 'robust', 'both', 'approaches', 'have', 'drawbacks', 'the', 'first', 'approach', 'has', 'computational', 'penalty', 'as', 'it', 'requires', 'optimising', 'non', 'convex', 'loss', 'the', 'second', 'approach', 'has', 'statistical', 'penalty', 'as', 'estimation', 'rates', 'with', 'rich', 'will', 'require', 'larger', 'sample', 'size', 'thus', 'it', 'appears', 'that', 'sln', 'robustness', 'involves', 'computational', 'statistical', 'tradeoff', 'however', 'there', 'is', 'variant', 'of', 'the', 'first', 'option', 'pick', 'loss', 'that', 'is', 'convex', 'but', 'not', 'convex', 'potential', 'such', 'loss', 'would', 'afford', 'the', 'computational', 'and', 'statistical', 'advantages', 'of', 'minimising', 'convex', 'risks', 'with', 'linear', 'scorers', 'manwani', 'and', 'sastry', 'demonstrated', 'that', 'square', 'loss', 'yv', 'is', 'one', 'such', 'loss', 'we', 'will', 'show', 'that', 'there', 'is', 'simpler', 'loss', 'that', 'is', 'convex', 'and', 'sln', 'robust', 'but', 'is', 'not', 'in', 'the', 'class', 'of', 'convex', 'potentials', 'by', 'virtue', 'of', 'being', 'negatively', 'unbounded', 'to', 'derive', 'this', 'loss', 'we', 'first', 're', 'interpret', 'robustness', 'via', 'noise', 'correction', 'procedure', 'even', 'if', 'we', 'were', 'content', 'with', 'difference', 'of', 'between', 'the', 'clean', 'and', 'corrupted', 'minimisers', 'performance', 'long', 'and', 'servedio', 'theorem', 'implies', 'that', 'in', 'the', 'worst', 'case', 'noise', 'corrected', 'loss', 'perspective', 'on', 'sln', 'robustness', 'we', 'now', 're', 'express', 'sln', 'robustness', 'to', 'reason', 'about', 'optimal', 'scorers', 'on', 'the', 'same', 'distribution', 'but', 'with', 'two', 'different', 'losses', 'this', 'will', 'help', 'characterise', 'set', 'of', 'strongly', 'sln', 'robust', 'losses', 'reformulating', 'sln', 'robustness', 'via', 'noise', 'corrected', 'losses', 'given', 'any', 'natarajan', 'et', 'al', 'lemma', 'showed', 'how', 'to', 'associate', 'with', 'loss', 'noise', 'corrected', 'counterpart', 'such', 'that', 'ld', 'the', 'loss', 'is', 'defined', 'as', 'follows', 'definition', 'noise', 'corrected', 'loss', 'given', 'any', 'loss', 'and', 'the', 'noise', 'corrected', 'loss', 'is', 'since', 'depends', 'on', 'the', 'unknown', 'parameter', 'it', 'is', 'not', 'directly', 'usable', 'to', 'design', 'an', 'sln', 'robust', 'learner', 'nonetheless', 'it', 'is', 'useful', 'theoretical', 'device', 'since', 'by', 'construction', 'for', 'any', 'sd', 'sd', 'sd', 'this', 'means', 'that', 'sufficient', 'condition', 'for', 'to', 'be', 'sln', 'robust', 'is', 'for', 'sd', 'ghosh', 'et', 'al', 'theorem', 'proved', 'sufficient', 'condition', 'on', 'such', 'that', 'this', 'holds', 'namely', 'interestingly', 'equation', 'is', 'necessary', 'for', 'stronger', 'notion', 'of', 'robustness', 'which', 'we', 'now', 'explore', 'characterising', 'stronger', 'notion', 'of', 'sln', 'robustness', 'as', 'the', 'first', 'step', 'towards', 'stronger', 'notion', 'of', 'robustness', 'we', 'rewrite', 'with', 'slight', 'abuse', 'of', 'notation', 'ld', 'where', 'is', 'distribution', 'over', 'labels', 'and', 'scores', 'standard', 'sln', 'robustness', 'requires', 'that', 'label', 'noise', 'does', 'not', 'change', 'the', 'risk', 'minimisers', 'that', 'if', 'is', 'such', 'that', 'for', 'all', 'the', 'same', 'relation', 'holds', 'with', 'in', 'place', 'of', 'strong', 'sln', 'robustness', 'strengthens', 'this', 'notion', 'by', 'requiring', 'that', 'label', 'noise', 'does', 'not', 'affect', 'the', 'ordering', 'of', 'all', 'pairs', 'of', 'joint', 'distributions', 'over', 'labels', 'and', 'scores', 'this', 'of', 'course', 'trivially', 'implies', 'sln', 'robustness', 'as', 'with', 'the', 'definition', 'of', 'given', 'distribution', 'over', 'labels', 'and', 'scores', 'let', 'be', 'the', 'corresponding', 'distribution', 'where', 'labels', 'are', 'flipped', 'with', 'probability', 'strong', 'sln', 'robustness', 'can', 'then', 'be', 'made', 'precise', 'as', 'follows', 'definition', 'strong', 'sln', 'robustness', 'call', 'loss', 'strongly', 'sln', 'robust', 'if', 'for', 'every', 'we', 'now', 're', 'express', 'strong', 'sln', 'robustness', 'using', 'notion', 'of', 'order', 'equivalence', 'of', 'loss', 'pairs', 'which', 'simply', 'requires', 'that', 'two', 'losses', 'order', 'all', 'distributions', 'over', 'labels', 'and', 'scores', 'identically', 'order', 'equivalent', 'if', 'definition', 'order', 'equivalent', 'loss', 'pairs', 'call', 'pair', 'of', 'losses', 'clearly', 'order', 'equivalence', 'of', 'implies', 'sd', 'sd', 'which', 'in', 'turn', 'implies', 'sln', 'robustness', 'it', 'is', 'thus', 'not', 'surprising', 'that', 'we', 'can', 'relate', 'order', 'equivalence', 'to', 'strong', 'sln', 'robustness', 'of', 'proposition', 'loss', 'is', 'strongly', 'sln', 'robust', 'iff', 'for', 'every', 'are', 'order', 'equivalent', 'this', 'connection', 'now', 'lets', 'us', 'exploit', 'classical', 'result', 'in', 'decision', 'theory', 'about', 'order', 'equivalent', 'losses', 'being', 'affine', 'transformations', 'of', 'each', 'other', 'combined', 'with', 'the', 'definition', 'of', 'this', 'lets', 'us', 'conclude', 'that', 'the', 'sufficient', 'condition', 'of', 'equation', 'is', 'also', 'necessary', 'for', 'strong', 'sln', 'robustness', 'of', 'proposition', 'loss', 'is', 'strongly', 'sln', 'robust', 'if', 'and', 'only', 'if', 'it', 'satisfies', 'equation', 'we', 'now', 'return', 'to', 'our', 'original', 'goal', 'which', 'was', 'to', 'find', 'convex', 'that', 'is', 'sln', 'robust', 'for', 'flin', 'and', 'ideally', 'more', 'general', 'function', 'classes', 'the', 'above', 'suggests', 'that', 'to', 'do', 'so', 'it', 'is', 'reasonable', 'to', 'consider', 'those', 'losses', 'that', 'satisfy', 'equation', 'unfortunately', 'it', 'is', 'evident', 'that', 'if', 'is', 'convex', 'non', 'constant', 'and', 'bounded', 'below', 'by', 'zero', 'then', 'it', 'cannot', 'possibly', 'be', 'admissible', 'in', 'this', 'sense', 'but', 'we', 'now', 'show', 'that', 'removing', 'the', 'boundedness', 'restriction', 'allows', 'for', 'the', 'existence', 'of', 'convex', 'admissible', 'loss', 'the', 'unhinged', 'loss', 'convex', 'strongly', 'sln', 'robust', 'loss', 'consider', 'the', 'following', 'simple', 'but', 'non', 'standard', 'convex', 'loss', 'unh', 'unh', 'and', 'compared', 'to', 'the', 'hinge', 'loss', 'the', 'loss', 'does', 'not', 'clamp', 'at', 'zero', 'it', 'does', 'not', 'have', 'hinge', 'thus', 'peculiarly', 'it', 'is', 'negatively', 'unbounded', 'an', 'issue', 'we', 'discuss', 'in', 'thus', 'we', 'call', 'this', 'the', 'unhinged', 'loss', 'the', 'loss', 'has', 'number', 'of', 'attractive', 'properties', 'the', 'most', 'immediate', 'being', 'is', 'its', 'sln', 'robustness', 'the', 'unhinged', 'loss', 'is', 'strongly', 'sln', 'robust', 'unh', 'unh', 'since', 'unh', 'is', 'strongly', 'sln', 'robust', 'and', 'thus', 'that', 'proposition', 'implies', 'that', 'unh', 'is', 'sln', 'robust', 'for', 'any', 'further', 'the', 'following', 'uniqueness', 'property', 'is', 'not', 'hard', 'to', 'show', 'proposition', 'pick', 'any', 'convex', 'loss', 'then', 'that', 'is', 'up', 'to', 'scaling', 'and', 'translation', 'unh', 'is', 'the', 'only', 'convex', 'loss', 'that', 'is', 'strongly', 'sln', 'robust', 'returning', 'to', 'the', 'case', 'of', 'linear', 'scorers', 'the', 'above', 'implies', 'that', 'unh', 'flin', 'is', 'sln', 'robust', 'this', 'does', 'not', 'contradict', 'proposition', 'since', 'unh', 'is', 'not', 'convex', 'potential', 'as', 'it', 'is', 'negatively', 'unbounded', 'intuitively', 'this', 'property', 'allows', 'the', 'loss', 'to', 'offset', 'the', 'penalty', 'incurred', 'by', 'instances', 'that', 'are', 'misclassified', 'with', 'high', 'margin', 'by', 'awarding', 'gain', 'for', 'instances', 'that', 'correctly', 'classified', 'with', 'high', 'margin', 'the', 'unhinged', 'loss', 'is', 'classification', 'calibrated', 'sln', 'robustness', 'is', 'by', 'itself', 'insufficient', 'for', 'learner', 'to', 'be', 'useful', 'for', 'example', 'loss', 'that', 'is', 'uniformly', 'zero', 'is', 'strongly', 'sln', 'robust', 'but', 'is', 'useless', 'as', 'it', 'is', 'not', 'classification', 'calibrated', 'fortunately', 'the', 'unhinged', 'loss', 'is', 'classification', 'calibrated', 'as', 'we', 'now', 'establish', 'for', 'technical', 'reasons', 'see', 'we', 'operate', 'with', 'fb', 'the', 'set', 'of', 'scorers', 'with', 'range', 'bounded', 'by', 'proposition', 'fix', 'unh', 'for', 'any', 'dm', 'fb', 'sign', 'thus', 'for', 'every', 'the', 'restricted', 'bayes', 'optimal', 'scorer', 'over', 'fb', 'has', 'the', 'same', 'sign', 'as', 'the', 'bayes', 'optimal', 'classifier', 'for', 'loss', 'in', 'the', 'limiting', 'case', 'where', 'rx', 'the', 'optimal', 'scorer', 'is', 'attainable', 'if', 'we', 'operate', 'over', 'the', 'extended', 'reals', 'so', 'that', 'unh', 'is', 'classification', 'calibrated', 'enforcing', 'boundedness', 'of', 'the', 'loss', 'while', 'the', 'classification', 'calibration', 'of', 'unh', 'is', 'encouraging', 'proposition', 'implies', 'that', 'its', 'unrestricted', 'bayes', 'risk', 'is', 'thus', 'the', 'regret', 'of', 'every', 'non', 'optimal', 'scorer', 'is', 'identically', 'which', 'hampers', 'analysis', 'of', 'consistency', 'in', 'orthodox', 'decision', 'theory', 'analogous', 'theoretical', 'issues', 'arise', 'when', 'attempting', 'to', 'establish', 'basic', 'theorems', 'with', 'unbounded', 'losses', 'ferguson', 'pg', 'we', 'can', 'side', 'step', 'this', 'issue', 'by', 'restricting', 'attention', 'to', 'bounded', 'scorers', 'so', 'that', 'unh', 'is', 'effectively', 'bounded', 'by', 'proposition', 'this', 'does', 'not', 'affect', 'the', 'classification', 'calibration', 'of', 'the', 'loss', 'in', 'the', 'context', 'of', 'linear', 'scorers', 'boundedness', 'of', 'scorers', 'can', 'be', 'achieved', 'by', 'regularisation', 'instead', 'of', 'work', 'ing', 'with', 'flin', 'one', 'can', 'instead', 'use', 'flin', 'hw', 'xi', 'where', 'so', 'that', 'flin', 'fr', 'for', 'supx', 'observe', 'that', 'as', 'unh', 'is', 'sln', 'robust', 'for', 'any', 'unh', 'flin', 'is', 'sln', 'robust', 'for', 'any', 'as', 'we', 'shall', 'see', 'in', 'working', 'with', 'flin', 'also', 'lets', 'us', 'establish', 'sln', 'robustness', 'of', 'the', 'hinge', 'loss', 'when', 'is', 'large', 'unhinged', 'loss', 'minimisation', 'on', 'corrupted', 'distribution', 'is', 'consistent', 'using', 'bounded', 'scorers', 'makes', 'it', 'possible', 'to', 'establish', 'surrogate', 'regret', 'bound', 'for', 'the', 'unhinged', 'loss', 'this', 'shows', 'classification', 'consistency', 'of', 'unhinged', 'loss', 'minimisation', 'on', 'the', 'corrupted', 'distribution', 'this', 'loss', 'has', 'been', 'considered', 'in', 'sriperumbudur', 'et', 'al', 'reid', 'and', 'williamson', 'in', 'the', 'context', 'of', 'maximum', 'mean', 'discrepancy', 'see', 'the', 'appendix', 'the', 'analysis', 'of', 'its', 'sln', 'robustness', 'is', 'to', 'our', 'knowledge', 'novel', 'proposition', 'fix', 'unh', 'then', 'for', 'any', 'and', 'scorer', 'fb', 'fb', 'regretd', 'regret', 'fb', 'regret', 'standard', 'rates', 'of', 'convergence', 'via', 'generalisation', 'bounds', 'are', 'also', 'trivial', 'to', 'derive', 'see', 'the', 'appendix', 'learning', 'with', 'the', 'unhinged', 'loss', 'and', 'kernels', 'we', 'now', 'show', 'that', 'the', 'optimal', 'solution', 'for', 'the', 'unhinged', 'loss', 'when', 'employing', 'regularisation', 'and', 'kernelised', 'scorers', 'has', 'simple', 'form', 'this', 'sheds', 'further', 'light', 'on', 'sln', 'robustness', 'and', 'regularisation', 'the', 'centroid', 'classifier', 'optimises', 'the', 'unhinged', 'loss', 'consider', 'minimising', 'the', 'unhinged', 'risk', 'over', 'the', 'class', 'of', 'kernelised', 'scorers', 'fh', 'hw', 'ih', 'for', 'some', 'where', 'is', 'feature', 'mapping', 'into', 'reproducing', 'kernel', 'hilbert', 'space', 'with', 'kernel', 'equivalently', 'given', 'distribution', 'we', 'want', 'wunh', 'argmin', 'hw', 'hw', 'wih', 'the', 'first', 'order', 'optimality', 'condition', 'implies', 'that', 'wunh', 'which', 'is', 'the', 'kernel', 'mean', 'map', 'of', 'smola', 'et', 'al', 'and', 'thus', 'the', 'optimal', 'unhinged', 'scorer', 'is', 'unh', 'from', 'equation', 'the', 'unhinged', 'solution', 'is', 'equivalent', 'to', 'nearest', 'centroid', 'classifier', 'manning', 'et', 'al', 'pg', 'tibshirani', 'et', 'al', 'shawe', 'taylor', 'and', 'cristianini', 'section', 'equation', 'gives', 'simple', 'way', 'to', 'understand', 'the', 'sln', 'robustness', 'of', 'unh', 'fh', 'as', 'the', 'optimal', 'scorers', 'on', 'the', 'clean', 'and', 'corrupted', 'distributions', 'only', 'differ', 'by', 'scaling', 'see', 'the', 'appendix', 'interestingly', 'servedio', 'theorem', 'established', 'that', 'nearest', 'centroid', 'classifier', 'which', 'they', 'termed', 'average', 'is', 'robust', 'to', 'general', 'class', 'of', 'label', 'noise', 'but', 'required', 'the', 'assumption', 'that', 'is', 'uniform', 'over', 'the', 'unit', 'sphere', 'our', 'result', 'establishes', 'that', 'sln', 'robustness', 'of', 'the', 'classifier', 'holds', 'without', 'any', 'assumptions', 'on', 'in', 'fact', 'ghosh', 'et', 'al', 'theorem', 'lets', 'one', 'quantify', 'the', 'unhinged', 'loss', 'performance', 'under', 'more', 'general', 'noise', 'model', 'see', 'the', 'appendix', 'for', 'discussion', 'practical', 'considerations', 'we', 'note', 'several', 'points', 'relating', 'to', 'practical', 'usage', 'of', 'the', 'unhinged', 'loss', 'with', 'kernelised', 'scorers', 'first', 'cross', 'validation', 'is', 'not', 'required', 'to', 'select', 'since', 'changing', 'only', 'changes', 'the', 'magnitude', 'of', 'scores', 'not', 'their', 'sign', 'thus', 'for', 'the', 'purposes', 'of', 'classification', 'one', 'can', 'simply', 'use', 'second', 'we', 'can', 'easily', 'extend', 'the', 'scorers', 'to', 'use', 'bias', 'regularised', 'with', 'strength', 'λb', 'tuning', 'λb', 'is', 'equivalent', 'to', 'computing', 'unh', 'as', 'per', 'equation', 'and', 'tuning', 'threshold', 'on', 'holdout', 'set', 'third', 'when', 'rd', 'for', 'small', 'we', 'can', 'store', 'wunh', 'explicitly', 'and', 'use', 'this', 'to', 'make', 'predictions', 'for', 'high', 'or', 'infinite', 'dimensional', 'we', 'can', 'either', 'make', 'predictions', 'directly', 'via', 'equation', 'or', 'use', 'random', 'fourier', 'features', 'rahimi', 'and', 'recht', 'to', 'approximately', 'embed', 'into', 'some', 'low', 'dimensional', 'rd', 'and', 'then', 'store', 'wunh', 'as', 'usual', 'the', 'latter', 'requires', 'translation', 'invariant', 'kernel', 'we', 'now', 'show', 'that', 'under', 'some', 'assumptions', 'wunh', 'coincides', 'with', 'the', 'solution', 'of', 'two', 'established', 'methods', 'the', 'appendix', 'discusses', 'some', 'further', 'relationships', 'to', 'the', 'maximum', 'mean', 'discrepancy', 'given', 'training', 'sample', 'dn', 'we', 'can', 'use', 'plugin', 'estimates', 'as', 'appropriate', 'equivalence', 'to', 'highly', 'regularised', 'svm', 'and', 'other', 'convex', 'potentials', 'there', 'is', 'an', 'interesting', 'equivalence', 'between', 'the', 'unhinged', 'solution', 'and', 'that', 'of', 'highly', 'regularised', 'svm', 'this', 'has', 'been', 'noted', 'in', 'hastie', 'et', 'al', 'section', 'which', 'showed', 'how', 'svms', 'approach', 'nearest', 'centroid', 'classifier', 'which', 'is', 'of', 'course', 'the', 'optimal', 'unhinged', 'solution', 'proposition', 'pick', 'any', 'and', 'with', 'supx', 'for', 'any', 'let', 'whinge', 'argmin', 'max', 'hw', 'ih', 'hw', 'wih', 'be', 'the', 'soft', 'margin', 'svm', 'solution', 'then', 'if', 'whinge', 'wunh', 'since', 'unh', 'fh', 'is', 'sln', 'robust', 'it', 'follows', 'that', 'for', 'hinge', 'max', 'yv', 'hinge', 'fh', 'is', 'similarly', 'sln', 'robust', 'provided', 'is', 'sufficiently', 'large', 'that', 'is', 'strong', 'regularisation', 'and', 'bounded', 'feature', 'map', 'endows', 'the', 'hinge', 'loss', 'with', 'sln', 'robustness', 'proposition', 'can', 'be', 'generalised', 'to', 'show', 'that', 'wunh', 'is', 'the', 'limiting', 'solution', 'of', 'any', 'twice', 'differentiable', 'convex', 'potential', 'this', 'shows', 'that', 'strong', 'regularisation', 'endows', 'most', 'learners', 'with', 'sln', 'robustness', 'intuitively', 'with', 'strong', 'regularisation', 'one', 'only', 'considers', 'the', 'behaviour', 'of', 'loss', 'near', 'zero', 'since', 'convex', 'potential', 'has', 'it', 'will', 'behave', 'similarly', 'to', 'its', 'linear', 'approximation', 'around', 'zero', 'viz', 'the', 'unhinged', 'loss', 'proposition', 'pick', 'any', 'bounded', 'feature', 'mapping', 'and', 'twice', 'differentiable', 'convex', 'potential', 'with', 'bounded', 'let', 'wφ', 'be', 'the', 'minimiser', 'of', 'the', 'regularised', 'risk', 'then', 'wunh', 'lim', 'wφ', 'wunh', 'equivalence', 'to', 'fisher', 'linear', 'discriminant', 'with', 'whitened', 'data', 'for', 'binary', 'classification', 'on', 'dm', 'the', 'fisher', 'linear', 'discriminant', 'fld', 'finds', 'weight', 'vector', 'proportional', 'to', 'the', 'minimiser', 'of', 'square', 'loss', 'sq', 'yv', 'bishop', 'section', 'wsq', 'ex', 'xxt', 'λi', 'wsq', 'is', 'only', 'changed', 'by', 'scaling', 'by', 'equation', 'and', 'the', 'fact', 'that', 'the', 'corrupted', 'marginal', 'factor', 'under', 'label', 'noise', 'this', 'provides', 'an', 'alternate', 'proof', 'of', 'the', 'fact', 'that', 'sq', 'flin', 'is', 'sln', 'robust', 'manwani', 'and', 'sastry', 'theorem', 'clearly', 'the', 'unhinged', 'loss', 'solution', 'wunh', 'is', 'equivalent', 'to', 'the', 'fld', 'and', 'square', 'loss', 'solution', 'wsq', 'when', 'the', 'input', 'data', 'is', 'whitened', 'xx', 'with', 'well', 'specified', 'with', 'universal', 'kernel', 'both', 'the', 'unhinged', 'and', 'square', 'loss', 'asymptotically', 'recover', 'the', 'optimal', 'classifier', 'but', 'the', 'unhinged', 'loss', 'does', 'not', 'require', 'matrix', 'inversion', 'with', 'misspecified', 'one', 'cannot', 'in', 'general', 'argue', 'for', 'the', 'superiority', 'of', 'the', 'unhinged', 'loss', 'over', 'square', 'loss', 'or', 'vice', 'versa', 'as', 'there', 'is', 'no', 'universally', 'good', 'surrogate', 'to', 'the', 'loss', 'reid', 'and', 'williamson', 'appendix', 'the', 'appendix', 'illustrate', 'examples', 'where', 'both', 'losses', 'may', 'underperform', 'sln', 'robustness', 'of', 'unhinged', 'loss', 'empirical', 'illustration', 'we', 'now', 'illustrate', 'that', 'the', 'unhinged', 'loss', 'sln', 'robustness', 'is', 'empirically', 'manifest', 'we', 'reiterate', 'that', 'with', 'high', 'regularisation', 'the', 'unhinged', 'solution', 'is', 'equivalent', 'to', 'an', 'svm', 'and', 'in', 'the', 'limit', 'any', 'classification', 'calibrated', 'loss', 'solution', 'thus', 'we', 'do', 'not', 'aim', 'to', 'assert', 'that', 'the', 'unhinged', 'loss', 'is', 'better', 'than', 'other', 'losses', 'but', 'rather', 'to', 'demonstrate', 'that', 'its', 'sln', 'robustness', 'is', 'not', 'purely', 'theoretical', 'we', 'first', 'show', 'that', 'the', 'unhinged', 'risk', 'minimiser', 'performs', 'well', 'on', 'the', 'example', 'of', 'long', 'and', 'servedio', 'henceforth', 'ls', 'figure', 'shows', 'the', 'distribution', 'where', 'with', 'marginal', 'distribution', 'and', 'all', 'three', 'instances', 'are', 'positive', 'we', 'pick', 'the', 'unhinged', 'minimiser', 'perfectly', 'classifies', 'all', 'three', 'points', 'regardless', 'of', 'the', 'level', 'of', 'label', 'noise', 'figure', 'the', 'hinge', 'minimiser', 'is', 'perfect', 'when', 'there', 'is', 'no', 'noise', 'but', 'with', 'even', 'small', 'amount', 'of', 'noise', 'achieves', 'error', 'rate', 'long', 'and', 'servedio', 'section', 'show', 'that', 'regularisation', 'does', 'not', 'endow', 'sln', 'robustness', 'square', 'loss', 'escapes', 'the', 'result', 'of', 'long', 'and', 'servedio', 'since', 'it', 'is', 'not', 'monotone', 'decreasing', 'unhinged', 'hinge', 'noise', 'hinge', 'noise', 'hinge', 'logistic', 'unhinged', 'table', 'mean', 'and', 'standard', 'deviation', 'of', 'the', 'error', 'over', 'trials', 'on', 'ls', 'grayed', 'cells', 'denote', 'the', 'best', 'performer', 'at', 'that', 'noise', 'rate', 'figure', 'ls', 'dataset', 'we', 'next', 'consider', 'empirical', 'risk', 'minimisers', 'from', 'random', 'training', 'sample', 'we', 'construct', 'training', 'set', 'of', 'instances', 'injected', 'with', 'varying', 'levels', 'of', 'label', 'noise', 'and', 'evaluate', 'classification', 'performance', 'on', 'test', 'set', 'of', 'instances', 'we', 'compare', 'the', 'hinge', 'logistic', 'for', 'ding', 'and', 'vishwanathan', 'and', 'unhinged', 'minimisers', 'using', 'linear', 'scorer', 'without', 'bias', 'term', 'and', 'regularisation', 'strength', 'from', 'table', 'even', 'at', 'label', 'noise', 'the', 'unhinged', 'classifier', 'is', 'able', 'to', 'find', 'perfect', 'solution', 'by', 'contrast', 'both', 'other', 'losses', 'suffer', 'at', 'even', 'moderate', 'noise', 'rates', 'we', 'next', 'report', 'results', 'on', 'some', 'uci', 'datasets', 'where', 'we', 'additionally', 'tune', 'threshold', 'so', 'as', 'to', 'ensure', 'the', 'best', 'training', 'set', 'accuracy', 'table', 'summarises', 'results', 'on', 'sample', 'of', 'four', 'datasets', 'the', 'appendix', 'contains', 'results', 'with', 'more', 'datasets', 'performance', 'metrics', 'and', 'losses', 'even', 'at', 'noise', 'close', 'to', 'the', 'unhinged', 'loss', 'is', 'often', 'able', 'to', 'learn', 'classifier', 'with', 'some', 'discriminative', 'power', 'hinge', 'logistic', 'unhinged', 'hinge', 'logistic', 'unhinged', 'iris', 'housing', 'hinge', 'logistic', 'unhinged', 'usps', 'hinge', 'logistic', 'unhinged', 'splice', 'table', 'mean', 'and', 'standard', 'deviation', 'of', 'the', 'error', 'over', 'trials', 'on', 'uci', 'datasets', 'conclusion', 'and', 'future', 'work', 'we', 'proposed', 'convex', 'classification', 'calibrated', 'loss', 'proved', 'that', 'is', 'robust', 'to', 'symmetric', 'label', 'noise', 'sln', 'robust', 'showed', 'it', 'is', 'the', 'unique', 'loss', 'that', 'satisfies', 'notion', 'of', 'strong', 'sln', 'robustness', 'established', 'that', 'it', 'is', 'optimised', 'by', 'the', 'nearest', 'centroid', 'classifier', 'and', 'showed', 'that', 'most', 'convex', 'potentials', 'such', 'as', 'the', 'svm', 'are', 'also', 'sln', 'robust', 'when', 'highly', 'regularised', 'so', 'with', 'apologies', 'to', 'wilde', 'while', 'the', 'truth', 'is', 'rarely', 'pure', 'it', 'can', 'be', 'simple', 'acknowledgments', 'nicta', 'is', 'funded', 'by', 'the', 'australian', 'government', 'through', 'the', 'department', 'of', 'communications', 'and', 'the', 'australian', 'research', 'council', 'through', 'the', 'ict', 'centre', 'of', 'excellence', 'program', 'the', 'authors', 'thank', 'cheng', 'soon', 'ong', 'for', 'valuable', 'comments', 'on', 'draft', 'of', 'this', 'paper', 'references', 'dana', 'angluin', 'and', 'philip', 'laird', 'learning', 'from', 'noisy', 'examples', 'machine', 'learning', 'peter', 'bartlett', 'michael', 'jordan', 'and', 'jon', 'mcauliffe', 'convexity', 'classification', 'and', 'risk', 'bounds', 'journal', 'of', 'the', 'american', 'statistical', 'association', 'christopher', 'bishop', 'pattern', 'recognition', 'and', 'machine', 'learning', 'springer', 'verlag', 'new', 'york', 'inc', 'avrim', 'blum', 'and', 'tom', 'mitchell', 'combining', 'labeled', 'and', 'unlabeled', 'data', 'with', 'co', 'training', 'in', 'conference', 'on', 'computational', 'learning', 'theory', 'colt', 'pages', 'vasil', 'denchev', 'nan', 'ding', 'hartmut', 'neven', 'and', 'vishwanathan', 'robust', 'classification', 'with', 'adiabatic', 'quantum', 'optimization', 'in', 'international', 'conference', 'on', 'machine', 'learning', 'icml', 'pages', 'luc', 'devroye', 'la', 'szlo', 'gyo', 'rfi', 'and', 'ga', 'bor', 'lugosi', 'probabilistic', 'theory', 'of', 'pattern', 'recognition', 'springer', 'nan', 'ding', 'and', 'vishwanathan', 'logistic', 'regression', 'in', 'advances', 'in', 'neural', 'information', 'processing', 'systems', 'nips', 'pages', 'curran', 'associates', 'inc', 'thomas', 'ferguson', 'mathematical', 'statistics', 'decision', 'theoretic', 'approach', 'academic', 'press', 'aritra', 'ghosh', 'naresh', 'manwani', 'and', 'sastry', 'making', 'risk', 'minimization', 'tolerant', 'to', 'label', 'noise', 'neurocomputing', 'trevor', 'hastie', 'saharon', 'rosset', 'robert', 'tibshirani', 'and', 'ji', 'zhu', 'the', 'entire', 'regularization', 'path', 'for', 'the', 'support', 'vector', 'machine', 'journal', 'of', 'machine', 'learning', 'research', 'december', 'issn', 'michael', 'kearns', 'efficient', 'noise', 'tolerant', 'learning', 'from', 'statistical', 'queries', 'journal', 'of', 'the', 'acm', 'november', 'philip', 'long', 'and', 'rocco', 'servedio', 'random', 'classification', 'noise', 'defeats', 'all', 'convex', 'potential', 'boosters', 'machine', 'learning', 'issn', 'christopher', 'manning', 'prabhakar', 'raghavan', 'and', 'hinrich', 'schu', 'tze', 'introduction', 'to', 'information', 'retrieval', 'cambridge', 'university', 'press', 'new', 'york', 'ny', 'usa', 'isbn', 'naresh', 'manwani', 'and', 'sastry', 'noise', 'tolerance', 'under', 'risk', 'minimization', 'ieee', 'transactions', 'on', 'cybernetics', 'june', 'hamed', 'masnadi', 'shirazi', 'vijay', 'mahadevan', 'and', 'nuno', 'vasconcelos', 'on', 'the', 'design', 'of', 'robust', 'classifiers', 'for', 'computer', 'vision', 'in', 'ieee', 'conference', 'on', 'computer', 'vision', 'and', 'pattern', 'recognition', 'cvpr', 'nagarajan', 'natarajan', 'inderjit', 'dhillon', 'pradeep', 'ravikumar', 'and', 'ambuj', 'tewari', 'learning', 'with', 'noisy', 'labels', 'in', 'advances', 'in', 'neural', 'information', 'processing', 'systems', 'nips', 'pages', 'ali', 'rahimi', 'and', 'benjamin', 'recht', 'random', 'features', 'for', 'large', 'scale', 'kernel', 'machines', 'in', 'advances', 'in', 'neural', 'information', 'processing', 'systems', 'nips', 'pages', 'mark', 'reid', 'and', 'robert', 'williamson', 'composite', 'binary', 'losses', 'journal', 'of', 'machine', 'learning', 'research', 'december', 'mark', 'reid', 'and', 'robert', 'williamson', 'information', 'divergence', 'and', 'risk', 'for', 'binary', 'experiments', 'journal', 'of', 'machine', 'learning', 'research', 'mar', 'bernhard', 'scho', 'lkopf', 'and', 'alexander', 'smola', 'learning', 'with', 'kernels', 'volume', 'mit', 'press', 'rocco', 'servedio', 'on', 'pac', 'learning', 'using', 'winnow', 'perceptron', 'and', 'perceptron', 'like', 'algorithm', 'in', 'conference', 'on', 'computational', 'learning', 'theory', 'colt', 'john', 'shawe', 'taylor', 'and', 'nello', 'cristianini', 'kernel', 'methods', 'for', 'pattern', 'analysis', 'cambridge', 'uni', 'press', 'alex', 'smola', 'arthur', 'gretton', 'le', 'song', 'and', 'bernhard', 'scho', 'lkopf', 'hilbert', 'space', 'embedding', 'for', 'distributions', 'in', 'algorithmic', 'learning', 'theory', 'alt', 'bharath', 'sriperumbudur', 'kenji', 'fukumizu', 'arthur', 'gretton', 'gert', 'lanckriet', 'and', 'bernhard', 'scho', 'lkopf', 'kernel', 'choice', 'and', 'classifiability', 'for', 'rkhs', 'embeddings', 'of', 'probability', 'distributions', 'in', 'advances', 'in', 'neural', 'information', 'processing', 'systems', 'nips', 'guillaume', 'stempfel', 'and', 'liva', 'ralaivola', 'learning', 'svms', 'from', 'sloppily', 'labeled', 'data', 'in', 'artificial', 'neural', 'networks', 'icann', 'volume', 'pages', 'springer', 'berlin', 'heidelberg', 'robert', 'tibshirani', 'trevor', 'hastie', 'balasubramanian', 'narasimhan', 'and', 'gilbert', 'chu', 'diagnosis', 'of', 'multiple', 'cancer', 'types', 'by', 'shrunken', 'centroids', 'of', 'gene', 'expression', 'proceedings', 'of', 'the', 'national', 'academy', 'of', 'sciences', 'oscar', 'wilde', 'the', 'importance', 'of', 'being', 'earnest']\n"
     ]
    }
   ],
   "source": [
    "documents = list (read_line(data['PaperText']))\n",
    "print(documents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "1894649b628cf7a1b9d0c96ab06504cf9209fe1f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-17 16:48:29,840 : INFO : collecting all words and their counts\n",
      "2019-03-17 16:48:29,842 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-03-17 16:48:30,238 : INFO : collected 37245 word types from a corpus of 1835093 raw words and 403 sentences\n",
      "2019-03-17 16:48:30,240 : INFO : Loading a fresh vocabulary\n",
      "2019-03-17 16:48:30,379 : INFO : effective_min_count=2 retains 23104 unique words (62% of original 37245, drops 14141)\n",
      "2019-03-17 16:48:30,380 : INFO : effective_min_count=2 leaves 1820952 word corpus (99% of original 1835093, drops 14141)\n",
      "2019-03-17 16:48:30,473 : INFO : deleting the raw counts dictionary of 37245 items\n",
      "2019-03-17 16:48:30,477 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2019-03-17 16:48:30,479 : INFO : downsampling leaves estimated 1434722 word corpus (78.8% of prior 1820952)\n",
      "2019-03-17 16:48:30,593 : INFO : estimated required memory for 23104 words and 200 dimensions: 48518400 bytes\n",
      "2019-03-17 16:48:30,594 : INFO : resetting layer weights\n",
      "2019-03-17 16:48:30,883 : INFO : training model with 10 workers on 23104 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-03-17 16:48:31,905 : INFO : EPOCH 1 - PROGRESS: at 41.19% examples, 577436 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-17 16:48:32,907 : INFO : EPOCH 1 - PROGRESS: at 88.34% examples, 626651 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-17 16:48:33,095 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-17 16:48:33,102 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-17 16:48:33,114 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-17 16:48:33,115 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-17 16:48:33,121 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-17 16:48:33,128 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-17 16:48:33,141 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-17 16:48:33,147 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-17 16:48:33,155 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 16:48:33,160 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 16:48:33,161 : INFO : EPOCH - 1 : training on 1835093 raw words (1434493 effective words) took 2.3s, 631644 effective words/s\n",
      "2019-03-17 16:48:34,185 : INFO : EPOCH 2 - PROGRESS: at 43.18% examples, 605945 words/s, in_qsize 19, out_qsize 3\n",
      "2019-03-17 16:48:35,185 : INFO : EPOCH 2 - PROGRESS: at 89.08% examples, 632212 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-17 16:48:35,349 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-17 16:48:35,361 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-17 16:48:35,370 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-17 16:48:35,371 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-17 16:48:35,372 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-17 16:48:35,383 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-17 16:48:35,392 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-17 16:48:35,400 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-17 16:48:35,406 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 16:48:35,410 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 16:48:35,411 : INFO : EPOCH - 2 : training on 1835093 raw words (1435320 effective words) took 2.2s, 639544 effective words/s\n",
      "2019-03-17 16:48:36,432 : INFO : EPOCH 3 - PROGRESS: at 43.18% examples, 603031 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-17 16:48:37,452 : INFO : EPOCH 3 - PROGRESS: at 88.09% examples, 619905 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-17 16:48:37,602 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-17 16:48:37,604 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-17 16:48:37,627 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-17 16:48:37,631 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-17 16:48:37,645 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-17 16:48:37,651 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-17 16:48:37,662 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-17 16:48:37,665 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-17 16:48:37,667 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 16:48:37,670 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 16:48:37,671 : INFO : EPOCH - 3 : training on 1835093 raw words (1435106 effective words) took 2.3s, 636499 effective words/s\n",
      "2019-03-17 16:48:38,678 : INFO : EPOCH 4 - PROGRESS: at 43.67% examples, 620445 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-17 16:48:39,706 : INFO : EPOCH 4 - PROGRESS: at 90.07% examples, 635838 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-17 16:48:39,857 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-17 16:48:39,859 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-17 16:48:39,861 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-17 16:48:39,872 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-17 16:48:39,874 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-17 16:48:39,876 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-17 16:48:39,891 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-17 16:48:39,897 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-17 16:48:39,906 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 16:48:39,907 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 16:48:39,908 : INFO : EPOCH - 4 : training on 1835093 raw words (1434542 effective words) took 2.2s, 642740 effective words/s\n",
      "2019-03-17 16:48:40,926 : INFO : EPOCH 5 - PROGRESS: at 44.17% examples, 621279 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-17 16:48:41,945 : INFO : EPOCH 5 - PROGRESS: at 87.10% examples, 613579 words/s, in_qsize 20, out_qsize 1\n",
      "2019-03-17 16:48:42,131 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-17 16:48:42,139 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-17 16:48:42,154 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-17 16:48:42,156 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-17 16:48:42,159 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-17 16:48:42,161 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-17 16:48:42,173 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-17 16:48:42,179 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-17 16:48:42,186 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 16:48:42,187 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 16:48:42,188 : INFO : EPOCH - 5 : training on 1835093 raw words (1434598 effective words) took 2.3s, 630873 effective words/s\n",
      "2019-03-17 16:48:42,189 : INFO : training on a 9175465 raw words (7174059 effective words) took 11.3s, 634598 effective words/s\n",
      "2019-03-17 16:48:42,190 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2019-03-17 16:48:42,190 : INFO : training model with 10 workers on 23104 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-03-17 16:48:43,211 : INFO : EPOCH 1 - PROGRESS: at 44.67% examples, 626643 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-17 16:48:44,222 : INFO : EPOCH 1 - PROGRESS: at 89.58% examples, 634821 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-17 16:48:44,335 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-17 16:48:44,344 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-17 16:48:44,365 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-17 16:48:44,368 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-17 16:48:44,373 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-17 16:48:44,389 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-17 16:48:44,402 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-17 16:48:44,415 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-17 16:48:44,420 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 16:48:44,422 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 16:48:44,422 : INFO : EPOCH - 1 : training on 1835093 raw words (1435056 effective words) took 2.2s, 645534 effective words/s\n",
      "2019-03-17 16:48:45,471 : INFO : EPOCH 2 - PROGRESS: at 41.69% examples, 567952 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-17 16:48:46,483 : INFO : EPOCH 2 - PROGRESS: at 85.61% examples, 595963 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-17 16:48:46,715 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-17 16:48:46,717 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-17 16:48:46,719 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-17 16:48:46,737 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-17 16:48:46,738 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-17 16:48:46,754 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-17 16:48:46,757 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-17 16:48:46,765 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-17 16:48:46,776 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 16:48:46,782 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 16:48:46,783 : INFO : EPOCH - 2 : training on 1835093 raw words (1434648 effective words) took 2.4s, 609224 effective words/s\n",
      "2019-03-17 16:48:47,812 : INFO : EPOCH 3 - PROGRESS: at 41.19% examples, 572414 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-17 16:48:48,821 : INFO : EPOCH 3 - PROGRESS: at 86.10% examples, 606233 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-17 16:48:49,065 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-17 16:48:49,068 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-17 16:48:49,069 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-17 16:48:49,093 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-17 16:48:49,095 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-17 16:48:49,096 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-17 16:48:49,108 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-17 16:48:49,129 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-17 16:48:49,132 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 16:48:49,136 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 16:48:49,137 : INFO : EPOCH - 3 : training on 1835093 raw words (1435042 effective words) took 2.3s, 611046 effective words/s\n",
      "2019-03-17 16:48:50,177 : INFO : EPOCH 4 - PROGRESS: at 41.69% examples, 573357 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-17 16:48:51,193 : INFO : EPOCH 4 - PROGRESS: at 84.86% examples, 591865 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-17 16:48:51,436 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-17 16:48:51,439 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-17 16:48:51,472 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-17 16:48:51,477 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-17 16:48:51,490 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-17 16:48:51,497 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-17 16:48:51,510 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-17 16:48:51,514 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-17 16:48:51,520 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 16:48:51,522 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 16:48:51,523 : INFO : EPOCH - 4 : training on 1835093 raw words (1434802 effective words) took 2.4s, 602828 effective words/s\n",
      "2019-03-17 16:48:52,542 : INFO : EPOCH 5 - PROGRESS: at 39.70% examples, 557837 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-17 16:48:53,544 : INFO : EPOCH 5 - PROGRESS: at 82.88% examples, 588298 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-17 16:48:53,840 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-17 16:48:53,853 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-17 16:48:53,868 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-17 16:48:53,874 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-17 16:48:53,887 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-17 16:48:53,889 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-17 16:48:53,901 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-17 16:48:53,910 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-17 16:48:53,921 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 16:48:53,923 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 16:48:53,924 : INFO : EPOCH - 5 : training on 1835093 raw words (1434815 effective words) took 2.4s, 599045 effective words/s\n",
      "2019-03-17 16:48:54,930 : INFO : EPOCH 6 - PROGRESS: at 40.20% examples, 572777 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-17 16:48:55,949 : INFO : EPOCH 6 - PROGRESS: at 85.36% examples, 604956 words/s, in_qsize 17, out_qsize 2\n",
      "2019-03-17 16:48:56,177 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-17 16:48:56,188 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-17 16:48:56,195 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-17 16:48:56,196 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-17 16:48:56,205 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-17 16:48:56,208 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-17 16:48:56,215 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-17 16:48:56,222 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-17 16:48:56,223 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 16:48:56,232 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 16:48:56,233 : INFO : EPOCH - 6 : training on 1835093 raw words (1434570 effective words) took 2.3s, 623067 effective words/s\n",
      "2019-03-17 16:48:57,246 : INFO : EPOCH 7 - PROGRESS: at 42.18% examples, 594236 words/s, in_qsize 17, out_qsize 2\n",
      "2019-03-17 16:48:58,252 : INFO : EPOCH 7 - PROGRESS: at 89.08% examples, 633373 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-17 16:48:58,426 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-17 16:48:58,435 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-17 16:48:58,441 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-17 16:48:58,456 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-17 16:48:58,460 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-17 16:48:58,461 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-17 16:48:58,474 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-17 16:48:58,475 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-17 16:48:58,482 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 16:48:58,492 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 16:48:58,493 : INFO : EPOCH - 7 : training on 1835093 raw words (1434800 effective words) took 2.3s, 636195 effective words/s\n",
      "2019-03-17 16:48:59,503 : INFO : EPOCH 8 - PROGRESS: at 44.67% examples, 631516 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-17 16:49:00,512 : INFO : EPOCH 8 - PROGRESS: at 89.08% examples, 633347 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-17 16:49:00,661 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-17 16:49:00,663 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-17 16:49:00,690 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-17 16:49:00,692 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-17 16:49:00,697 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-17 16:49:00,704 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-17 16:49:00,709 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-17 16:49:00,713 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-17 16:49:00,715 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 16:49:00,717 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 16:49:00,717 : INFO : EPOCH - 8 : training on 1835093 raw words (1434637 effective words) took 2.2s, 646626 effective words/s\n",
      "2019-03-17 16:49:01,735 : INFO : EPOCH 9 - PROGRESS: at 45.16% examples, 634719 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-17 16:49:02,741 : INFO : EPOCH 9 - PROGRESS: at 89.58% examples, 636651 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-17 16:49:02,854 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-17 16:49:02,865 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-17 16:49:02,866 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-17 16:49:02,916 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-17 16:49:02,924 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-17 16:49:02,929 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-17 16:49:02,930 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-17 16:49:02,935 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-17 16:49:02,937 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 16:49:02,942 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 16:49:02,943 : INFO : EPOCH - 9 : training on 1835093 raw words (1434756 effective words) took 2.2s, 646984 effective words/s\n",
      "2019-03-17 16:49:03,962 : INFO : EPOCH 10 - PROGRESS: at 43.18% examples, 605610 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-17 16:49:05,005 : INFO : EPOCH 10 - PROGRESS: at 90.57% examples, 630843 words/s, in_qsize 17, out_qsize 2\n",
      "2019-03-17 16:49:05,059 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-17 16:49:05,061 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-17 16:49:05,093 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-17 16:49:05,109 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-17 16:49:05,135 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-17 16:49:05,138 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-17 16:49:05,142 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-17 16:49:05,143 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-17 16:49:05,151 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 16:49:05,153 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 16:49:05,154 : INFO : EPOCH - 10 : training on 1835093 raw words (1433961 effective words) took 2.2s, 650400 effective words/s\n",
      "2019-03-17 16:49:05,154 : INFO : training on a 18350930 raw words (14347087 effective words) took 23.0s, 624779 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14347087, 18350930)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(\n",
    "        documents,\n",
    "        size=200,\n",
    "        window=10,\n",
    "        min_count=2,\n",
    "        workers=10)\n",
    "model.train(documents, total_examples=len(documents), epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "179923d5e2caa61b452272c198c86bc4c0d1754e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-17 16:49:05,268 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('extensive', 0.5657904744148254),\n",
       " ('understanding', 0.5527400374412537),\n",
       " ('developments', 0.5486680269241333),\n",
       " ('opens', 0.5451460480690002),\n",
       " ('progress', 0.5385216474533081),\n",
       " ('scientists', 0.5344961285591125),\n",
       " ('investigation', 0.5316309928894043),\n",
       " ('breakthroughs', 0.5298246145248413),\n",
       " ('studies', 0.5286712646484375),\n",
       " ('developing', 0.5261154174804688)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"development\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "341dad6ab6a9f03cfb5d3d071d4e16836140d268"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('perona', 0.7742531895637512),\n",
       " ('fish', 0.7723714113235474),\n",
       " ('horse', 0.7706104516983032),\n",
       " ('vondrick', 0.769294023513794),\n",
       " ('welinder', 0.7668565511703491),\n",
       " ('frisbee', 0.7664816379547119),\n",
       " ('comic', 0.7657363414764404),\n",
       " ('basal', 0.765304684638977),\n",
       " ('julius', 0.7632596492767334),\n",
       " ('sea', 0.7630102634429932)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"man\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "3b029387c92932af94889fa3036e62cd50faaeaf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('costume', 0.959667444229126),\n",
       " ('girl', 0.8462927341461182),\n",
       " ('julius', 0.7994595170021057),\n",
       " ('shaoqingren', 0.7970902323722839),\n",
       " ('scgb', 0.7940157055854797),\n",
       " ('faster_rcnn', 0.7921811938285828),\n",
       " ('sea', 0.7911180853843689),\n",
       " ('americans', 0.7891307473182678),\n",
       " ('chart', 0.7882951498031616),\n",
       " ('omj', 0.7880900502204895)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"woman\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "578edab5a228d29edfd4d6cc3a1ab285fde89158"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 0.9085661768913269),\n",
       " ('cow', 0.8652568459510803),\n",
       " ('horse', 0.8493620157241821),\n",
       " ('firehydrant', 0.8407943844795227),\n",
       " ('bottle', 0.8394757509231567),\n",
       " ('boat', 0.8227623701095581),\n",
       " ('bananas', 0.8137739896774292),\n",
       " ('sheep', 0.80582195520401),\n",
       " ('sofa', 0.8014547824859619),\n",
       " ('hydrant', 0.7759835124015808)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"dog\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "1a4247aee6d9769f5a8af8128bc7f8408e69bb0c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('christians', 0.9725555181503296),\n",
       " ('jesus', 0.8978714942932129),\n",
       " ('religion', 0.6373331546783447),\n",
       " ('φki', 0.6171691417694092),\n",
       " ('αpki', 0.6130654811859131),\n",
       " ('factory', 0.6121131181716919),\n",
       " ('χm', 0.6107220649719238),\n",
       " ('φkj', 0.6015666723251343),\n",
       " ('boy', 0.5992591381072998),\n",
       " ('bowl', 0.5966882109642029)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"god\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a6ad6e0815fb216592ce8d5145995334977b2775"
   },
   "source": [
    "Most of the results here are non sense, this is due to the corpora used for the training. Try this again with another training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "044b00211cf23fcf1af179ff89b67369a154492d"
   },
   "source": [
    "# Sentiment analysing based on tweets\n",
    "\n",
    "This project is inspired by [this blog](https://ahmedbesbes.com/sentiment-analysis-on-twitter-using-word2vec-and-keras.html) where based on a certain collection of tweets, we try to see whether or not their meaning is positive or negative.\n",
    "\n",
    "## Data\n",
    "\n",
    "The dataset is a dataset from Kaggle with a certain amount of tweets available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "a5f7c7b3b638280e3ac2a226ce734c3e87b22586"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../input/twitter-emotion/train.csv', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "34e9aa027bfdd36ca0c24678b0d39f714172c3fe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID                        ...                                                              SentimentText\n",
       "0       1                        ...                                               is so sad for my APL frie...\n",
       "1       2                        ...                                             I missed the New Moon trail...\n",
       "2       3                        ...                                                    omg its already 7:30 :O\n",
       "3       4                        ...                                    .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4       5                        ...                                   i think mi bf is cheating on me!!!   ...\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "36b6c7ff1cfb4f99ac70ed659147f0a584b43cfd"
   },
   "outputs": [],
   "source": [
    "#No need of the first row\n",
    "data = data[['Sentiment', 'SentimentText']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "5d97fac1f5cd2b4b6cae158bb0d3fd212c76561f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          0                       is so sad for my APL frie...\n",
       "1          0                     I missed the New Moon trail...\n",
       "2          1                            omg its already 7:30 :O\n",
       "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bea17825bff889d02b9d72b37f9262370ee21741"
   },
   "source": [
    "## Data cleaning\n",
    "\n",
    "Before using tweets, we need to clean them, this means mainly to get read of @ # and links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "390de793a0d31539699fd21e089cb45829ffde24"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "16895bf553c8edb36b11e34be779f80cbda6bdf9"
   },
   "outputs": [],
   "source": [
    "def tokenize(tweet):\n",
    "    try:\n",
    "        tweet = str(tweet.lower())\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "        tokens = [tok for tok in tokens if '#' not in tok]\n",
    "        tokens = [tok for tok in tokens if 'http' not in tok]\n",
    "        tokens = [tok for tok in tokens if '@' not in tok]\n",
    "        return tokens\n",
    "    except:\n",
    "        return 'NC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "7d673565dac7062408335c5fca6332f9298e4f59"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress-bar: 100%|██████████| 99989/99989 [00:05<00:00, 18536.18it/s]\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "def postprocess(data, n=1000000):\n",
    "    data = data.head(n)\n",
    "    data['tokens'] = data['SentimentText'].progress_map(tokenize)\n",
    "    data = data[data.tokens != 'NC']\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', inplace=True, axis=1)\n",
    "    return data\n",
    "\n",
    "data = postprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "78d36950d9123d973493c976d42683f44bea3f70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             [is, so, sad, for, my, apl, friend, ...]\n",
       "1            [i, missed, the, new, moon, trailer, ...]\n",
       "2                      [omg, its, already, 7:30, :, o]\n",
       "3    [.., omgaga, ., im, sooo, im, gunna, cry, ., i...\n",
       "4    [i, think, mi, bf, is, cheating, on, me, !, !,...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head().tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "12d89cc00d312cf67df68b83a9b79a8636757063"
   },
   "source": [
    "# Data preparation\n",
    "\n",
    "All the data are splitted into different sets to setup a training and test phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "672e36ef1ab4d0202d39e1ff632cede275ca4355"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(data.head(100000).tokens),\n",
    "                                                    np.array(data.head(100000).Sentiment), test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "dacee338f51b8fac092a5ab26c0377754fd281b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['well', 'said', '...', 'please', 'take', 'him', 'off', 'my', 'hands', '..', 'i', \"can't\", 'cope', '!', '!', '!', 'set', 'that', 'fox', 'on', 'him']),\n",
       "       list([\"that's\", 'all', 'i', 'have', 'to', 'say', 'or', 'show', 'lol']),\n",
       "       list(['kimora', 'congrats', 'to', 'you', 'and', 'djimon', 'on', 'your', 'new', 'baby', 'boy', '...', 'blessings']),\n",
       "       ...,\n",
       "       list(['speed', 'in', 'for', 'joe', 'again', 'check', 'out', 'the', 'article', 'i', 'posted', 'on', 'jnn']),\n",
       "       list(['mann', '.', 'i', 'wish', 'i', \"could've\", 'gone', '!']),\n",
       "       list(['glad', 'you', 'had', 'a', 'great', 'time', ',', 'but', \"i'm\", 'sad', 'too', 'it', 'was', 'just', 'amazing', 'last', 'night', '...', \"there's\", 'nothing', 'like', 'a', 'toronto', 'crowd', '!'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "ec92c797fffc5439167ede8211e67c40997e40a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  if __name__ == '__main__':\n",
      "79991it [00:00, 132203.34it/s]\n",
      "19998it [00:00, 55745.83it/s]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence\n",
    "\n",
    "def labelizeTweets(tweets, label_type):\n",
    "    labelized = []\n",
    "    for i,v in tqdm(enumerate(tweets)):\n",
    "        label = '%s_%s'%(label_type,i)\n",
    "        labelized.append(LabeledSentence(v, [label]))\n",
    "    return labelized\n",
    "\n",
    "x_train = labelizeTweets(x_train, 'TRAIN')\n",
    "x_test = labelizeTweets(x_test, 'TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "0a6d65663caee5dd87621775b14051d50d1d9c40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledSentence(words=['thunder', 'and', 'lightning', '.', 'one', 'thing', 'i', 'am', 'scared', 'of', '.', 'even', 'bear', 'is', 'scared'], tags=['TRAIN_10'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "5915be244178246f349b8c2b428c67a1512101bc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79991/79991 [00:00<00:00, 1501409.96it/s]\n",
      "2019-03-17 16:49:13,955 : INFO : collecting all words and their counts\n",
      "2019-03-17 16:49:13,956 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-03-17 16:49:13,998 : INFO : PROGRESS: at sentence #10000, processed 144097 words, keeping 13539 word types\n",
      "2019-03-17 16:49:14,042 : INFO : PROGRESS: at sentence #20000, processed 291532 words, keeping 21117 word types\n",
      "2019-03-17 16:49:14,090 : INFO : PROGRESS: at sentence #30000, processed 436064 words, keeping 27085 word types\n",
      "2019-03-17 16:49:14,140 : INFO : PROGRESS: at sentence #40000, processed 580320 words, keeping 32212 word types\n",
      "2019-03-17 16:49:14,188 : INFO : PROGRESS: at sentence #50000, processed 725958 words, keeping 37004 word types\n",
      "2019-03-17 16:49:14,235 : INFO : PROGRESS: at sentence #60000, processed 870812 words, keeping 41270 word types\n",
      "2019-03-17 16:49:14,281 : INFO : PROGRESS: at sentence #70000, processed 1015910 words, keeping 45386 word types\n",
      "2019-03-17 16:49:14,341 : INFO : collected 49036 word types from a corpus of 1161453 raw words and 79991 sentences\n",
      "2019-03-17 16:49:14,342 : INFO : Loading a fresh vocabulary\n",
      "2019-03-17 16:49:14,396 : INFO : effective_min_count=5 retains 9114 unique words (18% of original 49036, drops 39922)\n",
      "2019-03-17 16:49:14,398 : INFO : effective_min_count=5 leaves 1105286 word corpus (95% of original 1161453, drops 56167)\n",
      "2019-03-17 16:49:14,432 : INFO : deleting the raw counts dictionary of 49036 items\n",
      "2019-03-17 16:49:14,436 : INFO : sample=0.001 downsamples 58 most-common words\n",
      "2019-03-17 16:49:14,437 : INFO : downsampling leaves estimated 793929 word corpus (71.8% of prior 1105286)\n",
      "2019-03-17 16:49:14,472 : INFO : estimated required memory for 9114 words and 200 dimensions: 19139400 bytes\n",
      "2019-03-17 16:49:14,473 : INFO : resetting layer weights\n",
      "100%|██████████| 79991/79991 [00:00<00:00, 1512135.48it/s]\n",
      "100%|██████████| 79991/79991 [00:00<00:00, 1478139.26it/s]\n",
      "2019-03-17 16:49:14,704 : INFO : training model with 3 workers on 9114 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-03-17 16:49:15,723 : INFO : EPOCH 1 - PROGRESS: at 80.02% examples, 627631 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-17 16:49:15,943 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-17 16:49:15,948 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 16:49:15,951 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 16:49:15,952 : INFO : EPOCH - 1 : training on 1161453 raw words (793930 effective words) took 1.2s, 640016 effective words/s\n",
      "2019-03-17 16:49:16,965 : INFO : EPOCH 2 - PROGRESS: at 79.16% examples, 625039 words/s, in_qsize 6, out_qsize 1\n",
      "2019-03-17 16:49:17,199 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-17 16:49:17,203 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 16:49:17,209 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 16:49:17,210 : INFO : EPOCH - 2 : training on 1161453 raw words (794060 effective words) took 1.2s, 635416 effective words/s\n",
      "2019-03-17 16:49:18,234 : INFO : EPOCH 3 - PROGRESS: at 78.31% examples, 612248 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-17 16:49:18,480 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-17 16:49:18,482 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 16:49:18,484 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 16:49:18,485 : INFO : EPOCH - 3 : training on 1161453 raw words (793847 effective words) took 1.3s, 627152 effective words/s\n",
      "2019-03-17 16:49:19,498 : INFO : EPOCH 4 - PROGRESS: at 79.16% examples, 624721 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-17 16:49:19,738 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-17 16:49:19,743 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 16:49:19,749 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 16:49:19,750 : INFO : EPOCH - 4 : training on 1161453 raw words (794002 effective words) took 1.3s, 631538 effective words/s\n",
      "2019-03-17 16:49:20,782 : INFO : EPOCH 5 - PROGRESS: at 78.31% examples, 608930 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-17 16:49:21,019 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-17 16:49:21,024 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 16:49:21,033 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 16:49:21,034 : INFO : EPOCH - 5 : training on 1161453 raw words (793905 effective words) took 1.3s, 623828 effective words/s\n",
      "2019-03-17 16:49:22,053 : INFO : EPOCH 6 - PROGRESS: at 80.02% examples, 627548 words/s, in_qsize 6, out_qsize 0\n",
      "2019-03-17 16:49:22,269 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-17 16:49:22,278 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 16:49:22,282 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 16:49:22,283 : INFO : EPOCH - 6 : training on 1161453 raw words (793345 effective words) took 1.2s, 639287 effective words/s\n",
      "2019-03-17 16:49:23,294 : INFO : EPOCH 7 - PROGRESS: at 80.90% examples, 640077 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-17 16:49:23,501 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-17 16:49:23,505 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 16:49:23,514 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 16:49:23,515 : INFO : EPOCH - 7 : training on 1161453 raw words (793756 effective words) took 1.2s, 648408 effective words/s\n",
      "2019-03-17 16:49:24,526 : INFO : EPOCH 8 - PROGRESS: at 81.75% examples, 646689 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-17 16:49:24,713 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-17 16:49:24,720 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 16:49:24,726 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 16:49:24,727 : INFO : EPOCH - 8 : training on 1161453 raw words (793668 effective words) took 1.2s, 659405 effective words/s\n",
      "2019-03-17 16:49:25,738 : INFO : EPOCH 9 - PROGRESS: at 82.63% examples, 652777 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-17 16:49:25,926 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-17 16:49:25,928 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 16:49:25,937 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 16:49:25,938 : INFO : EPOCH - 9 : training on 1161453 raw words (793703 effective words) took 1.2s, 659190 effective words/s\n",
      "2019-03-17 16:49:26,948 : INFO : EPOCH 10 - PROGRESS: at 81.77% examples, 647573 words/s, in_qsize 5, out_qsize 0\n",
      "2019-03-17 16:49:27,145 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-17 16:49:27,149 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 16:49:27,157 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 16:49:27,158 : INFO : EPOCH - 10 : training on 1161453 raw words (794002 effective words) took 1.2s, 654863 effective words/s\n",
      "2019-03-17 16:49:27,159 : INFO : training on a 11614530 raw words (7938218 effective words) took 12.5s, 637381 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7938218, 11614530)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(size=200, min_count=5)\n",
    "model.build_vocab([x.words for x in tqdm(x_train)])\n",
    "model.train([x.words for x in tqdm(x_train)], total_examples=len([x.words for x in tqdm(x_train)]),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "bf91561783925bd230f4188a61c3dcdb12f001b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2019-03-17 16:49:27,210 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('cat', 0.7416350841522217),\n",
       " ('boyfriend', 0.7170374393463135),\n",
       " ('brother', 0.6902833580970764),\n",
       " ('kid', 0.6809959411621094),\n",
       " ('heart', 0.6758929491043091),\n",
       " ('teacher', 0.6732697486877441),\n",
       " ('wife', 0.6538675427436829),\n",
       " ('kitty', 0.6504315137863159),\n",
       " ('daddy', 0.6460946202278137),\n",
       " ('boy', 0.6455011963844299)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['vodka'])\n",
    "#model.most_similar(positive=['vodka', 'france'], negative=['russia'])\n",
    "#Paris\n",
    "w1 = \"man\"\n",
    "model.wv.most_similar (positive=w1)\n",
    "w1 = \"woman\"\n",
    "model.wv.most_similar (positive=w1)\n",
    "w1 = \"dog\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b1e6b93d3124d201a379e0f39fe4fec4b175eb29"
   },
   "source": [
    "Results now make more sense even if it may be surprising; this may be seen as the \"tweeter\" effect: tweets are short and quite explicite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "207a5b6e28d95e6d024dc24a0e6a908caef11197"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "11d3953abcf1d56da657cbc1126f1c7a99bb9185"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size : 5334\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x.words for x in x_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print ('vocab size :', len(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "cfe902dffaa624526618354721ec588c673b2a3c"
   },
   "outputs": [],
   "source": [
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "ae4c76dd0e6f238237b1338f636488539143e278"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "79991it [00:13, 5876.98it/s]\n",
      "0it [00:00, ?it/s]/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "19998it [00:03, 5629.70it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, 200) for z in tqdm(map(lambda x: x.words, x_train))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, 200) for z in tqdm(map(lambda x: x.words, x_test))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "0d27b234acda0b06f0d665acf6045f1ff6d28763"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9\n",
      " - 4s - loss: 0.5348 - acc: 0.7301\n",
      "Epoch 2/9\n",
      " - 3s - loss: 0.5132 - acc: 0.7450\n",
      "Epoch 3/9\n",
      " - 3s - loss: 0.5060 - acc: 0.7513\n",
      "Epoch 4/9\n",
      " - 3s - loss: 0.5023 - acc: 0.7535\n",
      "Epoch 5/9\n",
      " - 3s - loss: 0.4989 - acc: 0.7563\n",
      "Epoch 6/9\n",
      " - 3s - loss: 0.4962 - acc: 0.7580\n",
      "Epoch 7/9\n",
      " - 3s - loss: 0.4945 - acc: 0.7582\n",
      "Epoch 8/9\n",
      " - 3s - loss: 0.4923 - acc: 0.7598\n",
      "Epoch 9/9\n",
      " - 3s - loss: 0.4909 - acc: 0.7611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6d6e11bac8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "modelF = Sequential()\n",
    "modelF.add(Dense(32, activation='relu', input_dim=200))\n",
    "modelF.add(Dense(1, activation='sigmoid'))\n",
    "modelF.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "modelF.fit(train_vecs_w2v, y_train, epochs=9, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "fdabf41b817a8e9dbc17671483b704d4bfbfca7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7512751274948681\n",
      "LabeledSentence(['my', 'night', 'was', 'completely', 'devoid', 'of', 'lips', '.', 'at', 'least', 'lips', 'intended', 'for', 'me', '.', 'you', 'were', 'barely', 'alive', 'the', 'last', 'time', 'they', 'were', 'haha'], ['TEST_1']) 0\n"
     ]
    }
   ],
   "source": [
    "score = modelF.evaluate(test_vecs_w2v, y_test, batch_size=128, verbose=2)\n",
    "print (score[1])\n",
    "print(x_test[1], y_test[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "7624ee87e72f4fcd6c58c4d920a33ff3fb686ba7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
