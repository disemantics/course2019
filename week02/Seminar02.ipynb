{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b079034aea06433242b550b48341a3b0ef8d191a"
   },
   "source": [
    "# NSU Distributional Semantics 2019 Course. Seminar 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8f50a422cf0cca30e4c5a00979645c5f15da6bc6"
   },
   "source": [
    "In this seminar, we will learn how to build the simples count-based models for information retrieval tasks. We will ide a TED Talks dataset from Kaggle (https://www.kaggle.com/rounakbanik/ted-talks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e0217f96dfc7045acc8854238f5fb013230a4930"
   },
   "source": [
    "## Reading data\n",
    "At this step, we will open a dataset and select two text columns from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip ./data/ted-talks.zip -d ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "data_path = \"./data/ted_main.csv\"\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7ddcd8f132c66bf8e06f768b0046f8b6fc4d5d3e"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4b7ef0c14790af72ce1d2e432b64e5d5259ea1fa"
   },
   "outputs": [],
   "source": [
    "data = data[['description', 'main_speaker', 'name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4d77a6a32fbd4dff0f823cbc2bd347fbe6f7356c"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this if you don'd have enough memory\n",
    "data = data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a8f5a56f917d68050af3c2763311c79fa6484bb7"
   },
   "source": [
    "## Preparing data for processing\n",
    "At this step, we are going to prepare our text data. Preparation includes tokenization and stop-words filtering. Today we will omit the second step. We will also be working only with descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6d80ff40d54d15fb01772418b8c91b7c02b07648"
   },
   "source": [
    "To make a tokenization, we can simply use tokenizers from nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "61e3878eac648c36c572a2cc7a149584df870043"
   },
   "outputs": [],
   "source": [
    "from nltk import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "88b1ce48b522e869fa24f633cf919e9f24e704d8"
   },
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a047e20fa53eee2933c1440b8bb46cce0902a2b2"
   },
   "outputs": [],
   "source": [
    "descriptions = [tokenizer.tokenize(description.lower()) for description in data[\"description\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d13700708f8f197fd573ede38a14795a0fd2207f"
   },
   "outputs": [],
   "source": [
    "print(descriptions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8ac1bbfbe3082545386a801c84f1680c5a2508f8"
   },
   "source": [
    "## Converting texts to a Bag-of-Words format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c61657fd46770f6bcdfdea92867f858ac3b336fd"
   },
   "outputs": [],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f062ed56f99f68c9c43c256fee4d1468ce6af5ff"
   },
   "source": [
    "Here we're gonna use the default Dictionary function (but you can implement your own converter if you wish)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c2eaa5cb91b83242f6e75d211ebceb59e0cf2150"
   },
   "outputs": [],
   "source": [
    "corpora_dict = corpora.Dictionary(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "26260ee5674123579d22a9a976c9d23b1f8a7ad2"
   },
   "outputs": [],
   "source": [
    "print(list(corpora_dict.token2id.items())[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3d8b37351e4106b22b46f1a4f747688b17fb4c34"
   },
   "source": [
    "By default, id2token is empty. Let's fill this dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c9a5a59f88d78449c3a3509c90aede50950a3b38"
   },
   "outputs": [],
   "source": [
    "for token, token_id in corpora_dict.token2id.items():\n",
    "    corpora_dict.id2token[token_id] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b3f289cc7f250cf11d0f1cfde47c989848c80238"
   },
   "outputs": [],
   "source": [
    "print(list(corpora_dict.id2token.items())[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2f9059cb81d72f448b89772835cfe6b869ee0fd4"
   },
   "outputs": [],
   "source": [
    "len(corpora_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "45a067a7fed597bca4d41d5a16559d02c477be87"
   },
   "source": [
    "Now let's look at the BoW representation of an arbitrary sentense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4bc3c398f27d20d355fe96ef7d5824d6d5c62f86"
   },
   "outputs": [],
   "source": [
    "new_doc = \"Save trees in sake of ecology!\"\n",
    "new_vec = corpora_dict.doc2bow(tokenizer.tokenize(new_doc.lower()))\n",
    "print(new_vec)\n",
    "\n",
    "for word_id, _ in new_vec:\n",
    "    print(corpora_dict.id2token[word_id], end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1a1f9de27a0ed284c32f86007a8177d0604a76e3"
   },
   "source": [
    "Now let's do it with all texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cb52d4c599b817a6df74284cd61a2bf6a18dde45"
   },
   "outputs": [],
   "source": [
    "corpus = [corpora_dict.doc2bow(text) for text in descriptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "04a98c8019149a5c0e1fac589c27774117fa4541"
   },
   "outputs": [],
   "source": [
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "15fb2f8acb4db05cce2970dd712e28e341e9e95a"
   },
   "source": [
    "Now it's time to make a simple search machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1c0abe0e03103f0f17b65b6d229a2e27ab575d07"
   },
   "source": [
    "## BoW search machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c0c80314152c964ca81bd170fb12a3579b62441f"
   },
   "outputs": [],
   "source": [
    "from gensim import similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d787e23138e532934ce78ca4d1ea5e7c9fd92b6c"
   },
   "outputs": [],
   "source": [
    "index_bow = similarities.SparseMatrixSimilarity(corpus, num_features=len(corpora_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "12d2a6afec55f9f77099e795a54a65b7f9dcc897"
   },
   "outputs": [],
   "source": [
    "def search(index, query, top_n=5, prints=False):\n",
    "    \"\"\"\n",
    "    This function searches the most similar texts to the query.\n",
    "        :param index: gensim.similarities object\n",
    "        :param query: a string\n",
    "        :param top_n: how many variants it returns\n",
    "        :param prints: if True returns the results, otherwise prints the results\n",
    "        :returns: a list of tuples (matched_document_index, similarity_value)\n",
    "    \"\"\"\n",
    "    # getting a BoW vector\n",
    "    bow_vec = corpora_dict.doc2bow(query.lower().split())\n",
    "    similarities = index[bow_vec]  # get similarities between the query and all index documents\n",
    "    similarities = [(x, i) for i, x in enumerate(similarities)]\n",
    "    similarities.sort(key=lambda elem: -elem[0])  # sorting by similarity_value in decreasing order\n",
    "    res = []\n",
    "    if prints:\n",
    "        print(f\"{query}\\n\")\n",
    "    for result in similarities[:top_n]:\n",
    "        if prints:\n",
    "            print(f\"{data['description'][result[1]]} \\t {result[0]}\\n\")\n",
    "        else:\n",
    "            res.append((result[1], result[0]))\n",
    "    if not prints:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ef188d0a63b33943fa35101a3b607db151eb7b76"
   },
   "outputs": [],
   "source": [
    "search(index_bow, \"education system\", prints=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "14a68e281042688475858584c62c0ad5f0a955b2"
   },
   "outputs": [],
   "source": [
    "search(index_bow, \"healthy food\", prints=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d247b23ae896e65af4bfb26b3d9162718dc8c6f6"
   },
   "source": [
    "Seems like it works. But can our system search texts by citations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1232eb4107f6413a6c7e6e48488bcaa9136941fa"
   },
   "outputs": [],
   "source": [
    "search(index_bow, \"In an emotionally charged talk\", prints=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "57e0d226a098ced7c9be1443989e71227c53f8ba"
   },
   "source": [
    "Great! But what about searching by an annotation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c5a0b324bc85e0fd779b0f83004c8784a6dea1a4"
   },
   "outputs": [],
   "source": [
    "search(index_bow, \"Majora Carter: Greening the ghetto\", prints=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d4bca77a04c3c963406059fcd87792fc9606beec"
   },
   "source": [
    "Seems like our tagret document is not in top-5 results.\n",
    "\n",
    "On the next step, we will make more 'smart' model, TF-IDF model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f802fb8bd95a2419071c012b466d72b21b9b5b2d"
   },
   "source": [
    "## TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "48c4d368fb6f29b753b05c30749c99f3a2ae2815"
   },
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7f1cd55499a22e1dc52dde0e613bed88c0301e25"
   },
   "outputs": [],
   "source": [
    "model_tfidf = TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "75187c2381ff2c1da5c779f048800144b8d18040"
   },
   "outputs": [],
   "source": [
    "vector = model_tfidf[corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aa7b016726f470cf582434c83f823d6048a6603c"
   },
   "outputs": [],
   "source": [
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cd96856c23f5617b89bec22e3d049e802420e03e"
   },
   "outputs": [],
   "source": [
    "corpus_tfidf = model_tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7c737923358fefe1fd63bc2de8a1fa777c8957cd"
   },
   "outputs": [],
   "source": [
    "index_tfidf = similarities.SparseMatrixSimilarity(corpus_tfidf, num_features=len(corpora_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3e3d5b0fcf376c93079dd2a56d29fb785b7885be"
   },
   "outputs": [],
   "source": [
    "search(index_tfidf, \"Majora Carter: Greening the ghetto\", prints=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d07d7c7e4a17ce3287ea40e57d28076e3aa9d5bb"
   },
   "source": [
    "Much better! Now that we have Majora Carter talks in the top of the results. How many talks did she have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4f1082afd9d9f9b4f82bef3c1f9198f0ed2a92b8"
   },
   "outputs": [],
   "source": [
    "data[data[\"main_speaker\"] == \"Majora Carter\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3adf22c12c7157d4455886922179dea327fa4f66"
   },
   "source": [
    "Now, it's time to use dense vectors instead of sparse ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5c5269d7be3717876d7bee207c127eb5ad94bf93"
   },
   "source": [
    "## Doing SVD / LSA with your own hands\n",
    "\n",
    "This approach has a lot of names but it's main idea is quite simple: we try to approximate out source matrix by matrix of a lower rank. In this task, we will use original BoW matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b50f1c84f9c13475c14ecfef5dadc7f4a855b9dd"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "13e0ec70ac07e68b6a6ea00bcba15b4960175b10"
   },
   "outputs": [],
   "source": [
    "i_inds = []\n",
    "j_inds = []\n",
    "data_ij_values = []\n",
    "\n",
    "for i_ind, sparse_doc in enumerate(corpus):\n",
    "    for j_ind, data_ij in sparse_doc:\n",
    "        i_inds.append(i_ind)\n",
    "        j_inds.append(j_ind)\n",
    "        data_ij_values.append(data_ij)\n",
    "sparse_corpus = coo_matrix((data_ij_values, (i_inds, j_inds)))\n",
    "full_corpus = sparse_corpus.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7a3c2c318ad1bcefafb3f2fe69332620153fa042"
   },
   "source": [
    "sparse_corpus and full_corpus are matrices with sizes $N_{documents} \\times V$ where $V = len(vocabulary)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2fd42af241411955891727a92b65d7408e2554b4"
   },
   "outputs": [],
   "source": [
    "sparse_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "86c76fbe20a51ad8dd23fe116af1ede94c2e2ab4"
   },
   "outputs": [],
   "source": [
    "full_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "217a4a380aa5efccab34a00e85b6e25ce30bba97"
   },
   "source": [
    "We want to work with words as rows, so we have to transpose the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c8a8c16e31047713add4ee91b83b05de85e77efe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d29f87356b1eeea7458b202befad1fd8dc2fb5f6"
   },
   "outputs": [],
   "source": [
    "full_corpus = full_corpus.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bcc6b5d4904114c6f61ca4b941192598c40d6c99"
   },
   "outputs": [],
   "source": [
    "U, s, Vt = la.svd(full_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "784c0dfb25f526a8b47943f0aeeb597370d7a44d"
   },
   "outputs": [],
   "source": [
    "print(U.shape, s.shape, Vt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8755024bd90d5c3d64f09cb3c98bcabaad550956"
   },
   "source": [
    "Now we can choose how many singular values (s) we will take to approximate an original matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "21c76a0b1229c67f54bb1b258c427eae6e8d7502"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5b072bae8d1e4d2fd4e577deda6dd7445a746b0d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "plt.plot(np.arange(1, s.shape[0] + 1), s, label=\"singular values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dde44b1843aecd941cbc6528576b5c95df2a4d85"
   },
   "outputs": [],
   "source": [
    "rank_svd = 250\n",
    "\n",
    "U_trunced = U[:, :rank_svd]\n",
    "s_trunced = s[:rank_svd]\n",
    "Vt_trunced = Vt[:rank_svd, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "220a3536a602131b275c124087c5e2c3ff330a1e"
   },
   "outputs": [],
   "source": [
    "print(U_trunced.shape, s_trunced.shape, Vt_trunced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1fe5c8423163373df82beeeb1b97bd4083a06967"
   },
   "outputs": [],
   "source": [
    "corpus_lsa = U_trunced.dot(np.diag(s_trunced)).dot(Vt_trunced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b7dc108251403c1e4286f414585146adbe2119f9"
   },
   "outputs": [],
   "source": [
    "corpus_lsa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b391f7091a8d198747e32d396a729da1f3780b2b"
   },
   "outputs": [],
   "source": [
    "corpus_lsa[0][::50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "34dc631c6901fba0b084c2dd8b7e2d01212e5e31"
   },
   "source": [
    "Here you can run experiments on word similarity measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "22c47a76639a41056a0252d6bdd91f7c37c56d72"
   },
   "source": [
    "Back to documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "913cde9e37e6d6754c10728837cdcb11f48e5b93"
   },
   "outputs": [],
   "source": [
    "corpus_lsa = corpus_lsa.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "12aba77c840974bad1d1df0cbc4cbcc9b3c61f4b"
   },
   "outputs": [],
   "source": [
    "index_lsa_bow = similarities.MatrixSimilarity(corpus_lsa, num_features=len(corpora_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "658136e4f37573e60f47fb61d4ddd59459df2765"
   },
   "outputs": [],
   "source": [
    "search(index_lsa_bow, \"healthy food\", prints=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d9a8489c0a0353c73c98be060fe7b0379d6c254"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "56cc9843ff07527dde102c56b4676a514954afed"
   },
   "source": [
    "## LSI\n",
    "It is almost the same that we did in the previous section but this time we will used a built-in function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "57a7810b1e4e650c81da7f7d4ab36e96dcd9a57c"
   },
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ec820ff98f2bfc017f399e29f471b71221dbfc06"
   },
   "outputs": [],
   "source": [
    "model_lsi = LsiModel(corpus, id2word=corpora_dict.id2token, num_topics=rank_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "695231a77bdc5bddde79050b97a9bfd7c9d9704b"
   },
   "outputs": [],
   "source": [
    "model_lsi.print_topics(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "01451a8582d7a3a6a66a2f543c7fe3971e94a6d2"
   },
   "outputs": [],
   "source": [
    "for i in range(rank_svd):\n",
    "    print(i, model_lsi.projection.s[i], s_trunced[i], np.allclose(model_lsi.projection.s[i], s_trunced[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "15e1af71966215d5fb63a92d0e16a1f090376b9f"
   },
   "outputs": [],
   "source": [
    "corpus_lsi = model_lsi[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "255dec7509c583fbdaff2880d11c28cbd51b9f47"
   },
   "outputs": [],
   "source": [
    "len(corpus_lsi), len(corpus_lsi[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a6862a26bb8eadc526ba24d39758151ea2cdeaa4"
   },
   "outputs": [],
   "source": [
    "index_lsi_bow = similarities.MatrixSimilarity(corpus_lsi, num_features=len(corpora_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "131a11c78fe84f9642ae715335d65f518077de31"
   },
   "outputs": [],
   "source": [
    "search(index_lsi_bow, \"education system\", prints=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "002ef628ad082b8547192516c1ff48d1ab62694f"
   },
   "outputs": [],
   "source": [
    "search(index_lsi_bow, \"healthy food\", prints=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3f6e239f14d8293952f8217c2ba8421f2e16d66b"
   },
   "source": [
    "Can you explain why do we have zeros here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dcf22760835e70e56d9e64c045f61187fb8d3b24"
   },
   "source": [
    "# Homework (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "330e4ad0de97a8dbf2226534cdfe17a1ed42007e"
   },
   "source": [
    "## Your own Dictionary (2 points)\n",
    "\n",
    "Implement a class analogous to corpora.Dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "757c9ef9457c8d7185a45f2b5218d1aa2a99537f"
   },
   "outputs": [],
   "source": [
    "class MyDictionary():\n",
    "    def __init__(tokenized_texts):\n",
    "        self.token2id = dict()\n",
    "        self.id2token = dict()\n",
    "        # YOUR CODE HERE    \n",
    "    def doc2bow(tokenized_text):\n",
    "        # YOUR CODE HERE\n",
    "        return # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d80b3c61e1eed40b149c2c00eebc95de1bdb5e54"
   },
   "outputs": [],
   "source": [
    "test_corpus = [['hello', 'world'], ['hello']]\n",
    "my_dictionary = MyDictionary(test_corpus)\n",
    "for word in {'hello', 'world'}:\n",
    "    assert word in my_dictionary.token2id\n",
    "    assert my_dictionary.token2id[word] = my_dictionary.id2token[my_dictionary.token2id[word]]\n",
    "my_test_corpus_bow = [my_dictionary.doc2bow(text) for text in test_corpus] \n",
    "test_corpus_bow = [[(0, 1), (1, 1)], [(0, 1)]]\n",
    "assert my_test_corpus_bow == test_corpus_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ac778f6c665295bac8b5d790ad1ab4525892a247"
   },
   "source": [
    "## Deleting stopwords (4 points)\n",
    "\n",
    "In this task, you will clear our text corpur from stopwords and non-words like ',', '!)' etc. After that, build a new BoW and TF-IDF models. Make several queries to old and new systems and compare tre results. Did deleting stopwords really increased a quality of the search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": false,
    "_uuid": "f3da02521d69174e6f6c06f61f75e7b8a815a7ea"
   },
   "outputs": [],
   "source": [
    "# You may need regular expressions to check tokens on being real 'words'\n",
    "import re\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "clean_corpus = [] # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "148247f54a5e16d8b07f2164cd5f09a204f846ff"
   },
   "source": [
    "## Visualizing word embeddings (4 points)\n",
    "\n",
    "Given the example of visualizing BoW vectors on a 2D-plain, plot the same graphs for TF-IDF model without stopwords. Does distributional hipothesis work here? Explain your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "363dd9c819d3514b7211d07ede23860c43258b48"
   },
   "outputs": [],
   "source": [
    "import bokeh.models as bm, bokeh.plotting as pl\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
    "                 width=600, height=400, show=True, **kwargs):\n",
    "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
    "    if isinstance(color, str): color = [color] * len(x)\n",
    "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
    "\n",
    "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
    "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
    "    if show: pl.show(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "67ec650d13e460eb654a15bec43607792c66b59c"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "word_vectors_pca = PCA(n_components=2, random_state=4117).fit_transform(full_corpus)  # insert TF-IDF vectors here\n",
    "word_vectors_pca = preprocessing.scale(word_vectors_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "815c1797479c7de806493abf5ae7eae2d861a9da"
   },
   "outputs": [],
   "source": [
    "period = 50  # you can use 10 or 25 if it's ok for your computer\n",
    "\n",
    "words = [corpora_dict.id2token[i] for i in range(len(corpora_dict))][::period]\n",
    "draw_vectors(word_vectors_pca[:, 0][::period], word_vectors_pca[:, 1][::period], token=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "20959fc093ddf08c6d15222317184d11d0499613"
   },
   "source": [
    "The other way of projecting high-dimentional data on a 2D plain is t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6cfc1a9abdf0d7058c0f78ab6d2f29968cb67b78"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9a20efb0aabf2659865d043a10dc195b54454087"
   },
   "outputs": [],
   "source": [
    "word_tsne = TSNE(n_components=2, verbose=100).fit_transform(full_corpus[::period])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a8a51e648558a6aa3108a0f0cc03bdf6d1a78d60"
   },
   "outputs": [],
   "source": [
    "draw_vectors(word_tsne[:, 0], word_tsne[:, 1], color='green', token=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4e28d17ce13676fdcbe0f8da54acd7c5aa7e2f4b"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Tell what have you learned from this seminar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "29100b7a9d0111aae33223ed504a83bc4810b0ef"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
